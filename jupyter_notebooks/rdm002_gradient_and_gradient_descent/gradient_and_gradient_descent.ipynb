{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets talk about the following topics\n",
    "1. Derivative\n",
    "2. Derivative and partial derivative\n",
    "3. Derivative and directional derivative\n",
    "4. Derivative and gradient\n",
    "5. Gradient descent algorithm\n",
    "\n",
    "### Derivative\n",
    "<img src=\"./derivative_geometric_meaning.svg\">\n",
    "\n",
    "Definition of derivative:\n",
    "\n",
    "$$\n",
    "f'(x_0) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\\n",
    "\\lim_{\\Delta x \\to 0}\\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}\n",
    "$$\n",
    "\n",
    "$f'(x_0)$ indicates the changing rate/trend of $f(x)$ along the positive x-axis direction, more intuitively to say, to some point of x:\n",
    "  1. If $f'(x) > 0$, then the value of $f(x)$ will increase along the positive direction of x-axis;\n",
    "  2. If $f'(x) < 0$, then the value of $f(x)$ will decrease along the positive direction of x-axis;\n",
    "  \n",
    "### Derivative and partial derivative\n",
    "Definition of partial derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}f(x_0, x_1, \\dots, x_n) \\\n",
    "= \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} \\\n",
    "= \\lim_{\\Delta x \\to 0}\\frac{f(x_0, x_1, \\dots, x_i + \\Delta x, \\dots, x_n) - f(x_0, x_1, \\dots, x_i, \\dots, x_n)}{\\Delta x}\n",
    "$$\n",
    "\n",
    "So as you can see, derivative and partial derivative are essentially same, just:\n",
    "  1. Derivative indicates the changing rate/trend of unary function $f(x)$ along the positive x-axis direction;\n",
    "  2. Partial derivative $\\frac{\\partial}{\\partial x_i} (i = 0, 1, \\dots, n)$ indicates the changing rate/trend of multivariate function $f(x_0, x_1, \\dots, x_n)$ along the positive $x_i (i = 0, 1, \\dots, n)$ axis direction;\n",
    "  \n",
    "### Derivative and directional derivative\n",
    "Definition of directional derivative:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial}{\\partial v}f(x_0, x_1, \\dots, x_n) \\\n",
    "  &= \\lim_{v \\to 0}\\frac{\\Delta y}{\\Delta x} \\\n",
    "  = \\lim_{v \\to 0}\\frac{f(x_0 + \\Delta x_0, x_1 + \\Delta x_1, \\dots, x_i + \\Delta x_i, \\dots, x_n + \\Delta x_n)}{v} \\\\\n",
    "  \\newline\n",
    "  v &= \\sqrt{(\\Delta x_0)^2 + (\\Delta x_1)^2 + \\dots + (\\Delta x_i)^2 + \\dots + (\\Delta x_n)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In previous derivative and partial derivative definitions, we were all talking about the changing rate/trend of $f(x)$ along the axes, if we want to calculate the changing rate/trend of $f(x)$ along any arbitrary direction(360-degree), then the directional derivative comes into picture.\n",
    "\n",
    "That is: **directional derivative indicates the changing rate/trend of $f(x)$ along one arbitrary direction.**\n",
    "\n",
    "### Derivative and gradient\n",
    "Definition of gradient:\n",
    "\n",
    "$$\n",
    "grad\\,f(x_0, x_1, \\dots, x_n) = \\\n",
    "\\Big(\n",
    "  \\frac{\\partial f}{\\partial x_0}, \\\n",
    "  \\dots, \\\n",
    "  \\frac{\\partial f}{\\partial x_i}, \\\n",
    "  \\dots, \\\n",
    "  \\frac{\\partial f}{\\partial x_n}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "The notion of gradient is created only for answering one question: **in which direction of the function's independent variables space it has the maximum changing rate?**\n",
    "\n",
    "That is: the function's gradient at one point is such a vector:\n",
    "  1. It has the same direction with the directional derivative which has the maximum value at this point(at one point the directional derivative is 360-degree);\n",
    "  2. And its norm is the value of above point 1 described directional derivative;\n",
    "  \n",
    "Here pay attentions to the below three points:\n",
    "  1. Gradient is a vector, so it has both direction and magnitude;\n",
    "  2. It has the same direction with the directional derivative which has the maximum value at that point;\n",
    "  3. The value of the gradient is the value of above point 2 described directional derivative;\n",
    "  \n",
    "### Gradient descent algorithm\n",
    "Since one function has the maximum changing rate along the gradient's direction at one point of the independent variables space, then if we move along the **reversed** direction of the gradient direction, we can reduce the value of the cost function most efficiently.\n",
    "\n",
    "But how to move along the **reversed** direction of the gradient direction?\n",
    "  \n",
    "Since gradient and partial derivative are all vectors, then according to the rules of vector operating, we can just decrease correspondingly in each axis, that is:\n",
    "\n",
    "$\n",
    "\\text{repeat until convergence }\\Bigg\\{\n",
    "$\n",
    "\n",
    "$$\n",
    "x_0 = x_0 - \\alpha\\frac{\\partial f}{\\partial x_0} \\\\\n",
    "x_1 = x_1 - \\alpha\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "x_n = x_n - \\alpha\\frac{\\partial f}{\\partial x_n}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\Bigg\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit to [[机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent）](https://blog.csdn.net/walilk/article/details/50978864)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
