{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector operations\n",
    "\n",
    "#### Addition and subtraction\n",
    "- Let $\\vec{u} = (u_1, u_2)$ and $\\vec{v} = (v_1, v_2)$, then $\\vec{u} + \\vec{v} = (u_1 + v_1,\\, u_2 + v_2)$, $\\vec{u} - \\vec{v} = (u_1 - v_1,\\, u_2 - v_2)$\n",
    "\n",
    "<img src=\"./addition_subtraction.gif\">\n",
    "\n",
    "#### Dot product\n",
    "1. The dot product gives a number as an answer (a 'scalar', not a vector).\n",
    "2. The dot product is written using a central dot:\n",
    "$$\n",
    "\\vec{a} \\bullet \\vec{b}\n",
    "$$\n",
    "3. We can calculate the dot product of two vectors in this way:\n",
    "$$\n",
    "\\vec{a} \\bullet \\vec{b} = |\\vec{a}| |\\vec{b}| cos(\\theta)\n",
    "$$\n",
    "\n",
    "<img src=\"./dot_product.gif\">\n",
    "\n",
    "$$\n",
    "|\\vec{a}| \\text{ is the magnitude (length) of vector a} \\\\\n",
    "|\\vec{b}| \\text{ is the magnitude (length) of vector b} \\\\\n",
    "\\theta \\text{ is the angle between a and b}\n",
    "$$\n",
    "\n",
    "#### Cross product\n",
    "1. The cross product of $\\vec{a} \\times \\vec{b}$ is another vector that is at right angles to both:\n",
    "<img src=\"cross_product_0.gif\">\n",
    "<center>And it all happens in 3 dimensions!</center>\n",
    "    \n",
    "2. We can calculate the cross product in this way:\n",
    "$$\n",
    "\\vec{a} \\times \\vec{b} = |\\vec{a}| |\\vec{b}| sin(\\theta) \\vec{n}\n",
    "$$\n",
    "<img src=\"cross_product_1.gif\">\n",
    "$$\n",
    "|\\vec{a}| \\text{ is the magnitude (length) of vector a} \\\\\n",
    "|\\vec{b}| \\text{ is the magnitude (length) of vector b} \\\\\n",
    "\\theta \\text{ is the angle between vector a and vector b} \\\\\n",
    "\\vec{n} \\text{ is the unit vector at right angles to both vector a and vector b}\n",
    "$$\n",
    "\n",
    "3. Or we can calculate the cross product in another way:\n",
    "$$\n",
    "\\vec{a} = (a_x, a_y, a_z) \\\\\n",
    "\\vec{b} = (b_x, b_y, b_z)\n",
    "$$\n",
    "<img src=\"cross_product_2.jpeg\" width=\"50%\" height=\"auto\">\n",
    "$$\n",
    "\\vec{c} = \\vec{a} \\times \\vec{b} = (a_yb_z - a_zb_y,\\, a_zb_x - a_xb_z,\\, a_xb_y - a_yb_x)\n",
    "$$\n",
    "\n",
    "    - **Question: how do we extend this to the cross product of a four dimensional vector or more higher, like the right part of the above graph?**\n",
    "\n",
    "4. Which direction?\n",
    "  - The cross product could point in the completely opposite direction and still be at right angles to the two other vectors, so we have the **\"Right Hand Rule\"**:\n",
    "  \n",
    "    - With your right-hand, point your index finger along vector a, and point your middle finger along vector b: the cross product goes in the direction of your thumb.\n",
    "    - <img src=\"right_hand_rule.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal equation\n",
    "\n",
    "#### Lets use the same sample dataset from my another post: [Multivariable linear regression(gradient descent)](https://lnshi.github.io/ml-exercises/jupyter_notebooks_in_html/rdm001_multivariable_linear_regression_gradient_descent/multivariable_linear_regression_gradient_descent.html#Lets-say-we-have-sample-data-set:) to reveal the normal equation first:\n",
    "\n",
    "  - We still try to find the below fitting equation to minimise the $\\sum\\limits_{i=1}^n\\varepsilon_i^2$\n",
    "  \n",
    "    $$\n",
    "    y_\\theta(x_1, x_2, \\dots, x_m) = \\\n",
    "    \\theta^T \\begin{pmatrix}1 & x_1 & x_2 & \\dots & x_m\\end{pmatrix}, \\,\n",
    "    \\theta = \\begin{pmatrix} \\theta_0 & \\theta_1, & \\dots &, \\theta_m \\end{pmatrix}\n",
    "    \\\\\n",
    "    $$\n",
    "\n",
    "  - Normal equation:\n",
    "    \n",
    "    $$\n",
    "    \\text{Let matrix }X = \\\n",
    "    \\begin{pmatrix}\n",
    "      1 & x_1^{(1)} & x_2^{(1)} & \\dots & x_m^{(1)} \\\\\n",
    "      1 & x_1^{(2)} & x_2^{(2)} & \\dots & x_m^{(2)} \\\\\n",
    "      \\vdots \\\\\n",
    "      1 & x_1^{(n)} & x_2^{(n)} & \\dots & x_m^{(n)}\n",
    "    \\end{pmatrix}, \\,\n",
    "    \\text{and matrix } y = \\\n",
    "    \\begin{pmatrix}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      \\vdots \\\\\n",
    "      y_n\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\text{then matrix } \\theta = (X^TX)^{-1}X^Ty\n",
    "    $$\n",
    "    \n",
    "#### How do we get the normal equation?\n",
    "\n",
    "  1. Lets see one simplest example in $R^2$ space:\n",
    "    \n",
    "      Example 1: kike below figure, there are two vectors in $R^2$ space, try to find out a constant $\\theta$ to make $\\theta\\vec{a} = \\vec{b}$.\n",
    "      <img src=\"normal_equation_0.jpg\">\n",
    "      \n",
    "      Clearly there is no **perfect solution**, coz in $R^2$ space, vector a and b they are non-collinear.\n",
    "      \n",
    "      \n",
    "  2. Lets see another example in $R^3$ space:\n",
    "  \n",
    "      Example 2: like below figure, there are thres vectors in $R^3$ space, try to find out a combination of $\\theta_1 \\text{ and } \\theta_2$ to make $\\theta_1\\vec{a_1} + \\theta_2\\vec{a_2} = \\vec{b}$.\n",
    "      <img src=\"normal_equation_1.jpg\">\n",
    "      \n",
    "      Clearly this one has also no **perfection solution**, coz vector b is not in the plane which is decided by vector a and b.\n",
    "      \n",
    "      \n",
    "  3. In reality, nearly all cases will be like above two cases, there is no **perfect solution**, but how do we find a **best solution** to minimise the errors? Just like in 'multivariable linear regression' we are trying to find out the fiting equation to minisize the $\\sum\\limits_{i=1}^n\\varepsilon_i^2$.\n",
    "  \n",
    "  4. Projection\n",
    "  \n",
    "      - In above 'Example 1', the best solution is: we leave the $\\vec{b}$'s component which is vertical to $\\vec{a}$ alone, only consider its component which has same direction with $\\vec{a}$ ($\\vec{b}$'s vertical projection on $\\vec{a}$), that is: $\\vec{p} = \\theta^*\\vec{a}$, indicated in below figure:\n",
    "      <img src=\"normal_equation_2.jpg\">\n",
    "      \n",
    "      Then the original problem $\\theta\\vec{a} = \\vec{b}$ is converted to find a $\\theta^*$ to make $\\theta^*\\vec{a} = \\vec{a}$ ( **$\\theta^*$ is the estimator of $\\theta$** ).\n",
    "      \n",
    "      Since $\\vec{e} \\perp \\vec{a}$, then:\n",
    "      \n",
    "      $$\n",
    "      \\begin{align*}\n",
    "        &\\vec{a} \\bullet (\\vec{b} - \\vec{p}) = 0 \\\\\n",
    "        &\\Rightarrow \\vec{a} \\bullet (\\vec{b} - \\theta^*\\vec{a}) = 0 \\\\\n",
    "        &\\Rightarrow \\vec{a}^T \\bullet (\\vec{b} - \\theta^*\\vec{a}) = 0 \\\\\n",
    "        &\\Rightarrow \\vec{a}^T \\bullet \\vec{b} = \\theta^*\\vec{a}^T \\bullet \\vec{a} \\\\\n",
    "        &\\Rightarrow \\theta^* = \\frac{\\vec{a}^T \\bullet \\vec{b}}{\\vec{a}^T \\bullet \\vec{a}}\n",
    "      \\end{align*}\n",
    "      $$\n",
    "      \n",
    "      - Lets see in above 'Example 2' how do we extend the theory we just got:\n",
    "      \n",
    "      In above 'Example 2', the best solution is: we leave $\\vec{b}$'s component which is vertical to plane P alone, only consider its component which is inside plane P ( $\\vec{b}$'s vertical projection on plane P ), that is:\n",
    "      \n",
    "      $$\n",
    "      \\vec{p} = \\theta^*\\begin{pmatrix}\\vec{a_1} \\\\ \\vec{a_2}\\end{pmatrix}, \\, \\\n",
    "      \\theta^* = \\\n",
    "      \\begin{pmatrix}\n",
    "        \\theta_1^* & \\theta_2^*\n",
    "      \\end{pmatrix}\n",
    "      \\quad\n",
    "      (\\, \\text{that is: }\\vec{p} = \\theta_1^*\\vec{a_1} + \\theta_2^*\\vec{a_2} \\,)\n",
    "      $$\n",
    "      \n",
    "      indicated in below figure:\n",
    "      <img src=\"normal_equation_3.jpg\">\n",
    "      \n",
    "      Then the original problem $\\theta_1\\vec{a_1} + \\theta_2\\vec{a_2} = \\vec{b}$ is converted to find a $\\theta^* = \\begin{pmatrix}\\theta_1^* & \\theta_2^*\\end{pmatrix}$ to make $\\vec{p} = \\theta^*\\begin{pmatrix}a_1 \\\\ a_2\\end{pmatrix}$ ( **$\\theta^*$ is the estimator of $\\theta$** ).\n",
    "      \n",
    "      $$\n",
    "      \\text{Let matrix }A = \\begin{pmatrix}\\vec{a_1} \\\\ \\vec{a_2}\\end{pmatrix}, \\, \\\n",
    "      \\text{and matrix}\\theta^* = \\begin{pmatrix}\\theta_1^* & \\theta_2^*\\end{pmatrix}\n",
    "      $$\n",
    "      \n",
    "      We find the $\\theta\\vec{a} = \\vec{b}$ in $R^2$ now in $R^3$ is extended to $A\\vec{\\theta} = \\vec{b}$,<br>\n",
    "      and correspondingly the $\\theta^*\\vec{a} = \\vec{p}$ in $R^2$ now in $R^3$ is extended to $A\\vec{\\theta^*} = \\vec{p}$.\n",
    "      \n",
    "      Since $\\vec{e} \\perp \\vec{a}$, then:\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
