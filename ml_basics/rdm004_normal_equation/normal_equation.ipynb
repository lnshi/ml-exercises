{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector operations\n",
    "\n",
    "#### Addition and subtraction\n",
    "1. Let $\\vec{u} = (u_1, u_2)$ and $\\vec{v} = (v_1, v_2)$, then $\\vec{u} + \\vec{v} = (u_1 + v_1,\\, u_2 + v_2)$, $\\vec{u} - \\vec{v} = (u_1 - v_1,\\, u_2 - v_2)$\n",
    "\n",
    "<img src=\"./addition_subtraction.gif\">\n",
    "\n",
    "#### Dot product (scalar product, inner product)\n",
    "1. The dot product gives a number as an answer (a 'scalar', not a vector).\n",
    "2. The dot product is written using a central dot:\n",
    "$$\n",
    "\\vec{a} \\bullet \\vec{b}\n",
    "$$\n",
    "3. We can calculate the dot product of two vectors in this way:\n",
    "$$\n",
    "\\vec{a} \\bullet \\vec{b} = |\\vec{a}| |\\vec{b}| cos(\\theta)\n",
    "$$\n",
    "\n",
    "<img src=\"./dot_product.gif\">\n",
    "\n",
    "$$\n",
    "|\\vec{a}| \\text{ is the magnitude (length) of vector a} \\\\\n",
    "|\\vec{b}| \\text{ is the magnitude (length) of vector b} \\\\\n",
    "\\theta \\text{ is the angle between a and b}\n",
    "$$\n",
    "\n",
    "#### Cross product\n",
    "1. The cross product of $\\vec{a} \\times \\vec{b}$ is another vector that is at right angles to both:\n",
    "<img src=\"cross_product_0.gif\">\n",
    "<center>And it all happens in 3 dimensions!</center>\n",
    "    \n",
    "2. We can calculate the cross product in this way:\n",
    "$$\n",
    "\\vec{a} \\times \\vec{b} = |\\vec{a}| |\\vec{b}| sin(\\theta) \\vec{n}\n",
    "$$\n",
    "<img src=\"cross_product_1.gif\">\n",
    "$$\n",
    "|\\vec{a}| \\text{ is the magnitude (length) of vector a} \\\\\n",
    "|\\vec{b}| \\text{ is the magnitude (length) of vector b} \\\\\n",
    "\\theta \\text{ is the angle between vector a and vector b} \\\\\n",
    "\\vec{n} \\text{ is the unit vector at right angles to both vector a and vector b}\n",
    "$$\n",
    "\n",
    "3. Or we can calculate the cross product in another way:\n",
    "$$\n",
    "\\vec{a} = (a_x, a_y, a_z) \\\\\n",
    "\\vec{b} = (b_x, b_y, b_z)\n",
    "$$\n",
    "<img src=\"cross_product_2.jpeg\" width=\"50%\" height=\"auto\">\n",
    "$$\n",
    "\\vec{c} = \\vec{a} \\times \\vec{b} = (a_yb_z - a_zb_y,\\, a_zb_x - a_xb_z,\\, a_xb_y - a_yb_x)\n",
    "$$\n",
    "\n",
    "    **Question: how do we extend this to the cross product of a four dimensional vector or more higher, like the right part of the above graph?**\n",
    "\n",
    "\n",
    "4. Which direction?\n",
    "\n",
    "    The cross product could point in the completely opposite direction and still be at right angles to the two other vectors, so we have the **\"Right Hand Rule\"**:\n",
    "  \n",
    "        With your right-hand, point your index finger along vector a, and point your middle finger along vector b: the cross product goes in the direction of your thumb.\n",
    "    <img src=\"right_hand_rule.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal equation\n",
    "\n",
    "#### Lets use the same sample dataset from my another post: [Multivariable linear regression(gradient descent)](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm001_multivariable_linear_regression_gradient_descent/multivariable_linear_regression_gradient_descent.html#Lets-say-we-have-sample-data-set:) to reveal the normal equation first:\n",
    "\n",
    "1. We still try to find the below fitting equation to minimise the $\\sum\\limits_{i=1}^n\\varepsilon_i^2$\n",
    "  \n",
    "    $$\n",
    "    y_\\theta(x_1, x_2, \\dots, x_m) = \\\n",
    "    \\theta \\begin{pmatrix}1 & x_1 & x_2 & \\dots & x_m\\end{pmatrix}, \\,\n",
    "    \\theta = \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_m \\end{pmatrix}\n",
    "    \\\\\n",
    "    $$\n",
    "\n",
    "2. Normal equation:\n",
    "    \n",
    "    $$\n",
    "    \\text{Let matrix }X = \\\n",
    "    \\begin{pmatrix}\n",
    "      1 & x_1^{(1)} & x_2^{(1)} & \\dots & x_m^{(1)} \\\\\n",
    "      1 & x_1^{(2)} & x_2^{(2)} & \\dots & x_m^{(2)} \\\\\n",
    "      \\vdots \\\\\n",
    "      1 & x_1^{(n)} & x_2^{(n)} & \\dots & x_m^{(n)}\n",
    "    \\end{pmatrix}, \\,\n",
    "    \\text{and matrix } y = \\\n",
    "    \\begin{pmatrix}\n",
    "      y^{(1)} \\\\\n",
    "      y^{(2)} \\\\\n",
    "      \\vdots \\\\\n",
    "      y^{(n)}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\text{then matrix } \\theta = (X^TX)^{-1}X^Ty\n",
    "    $$\n",
    "    \n",
    "#### How do we get the normal equation?\n",
    "\n",
    "1. Lets see one simplest example in $R^2$ space:\n",
    "    \n",
    "    Example 1: kike below figure, there are two vectors in $R^2$ space, try to find out a constant $\\theta$ to make $\\theta\\vec{a} = \\vec{b}$.\n",
    "    <img src=\"normal_equation_0.jpg\">\n",
    "      \n",
    "    Clearly there is no **perfect solution**, coz in $R^2$ space, vector a and b they are non-collinear.\n",
    "      \n",
    "      \n",
    "2. Lets see another example in $R^3$ space:\n",
    "  \n",
    "    Example 2: like below figure, there are thres vectors in $R^3$ space, try to find out a combination of $\\theta_1 \\text{ and } \\theta_2$ to make $\\theta_1\\vec{a_1} + \\theta_2\\vec{a_2} = \\vec{b}$.\n",
    "    <img src=\"normal_equation_1.jpg\">\n",
    "      \n",
    "    Clearly this one has also no **perfection solution**, coz vector b is not in the plane which is decided by vector a and b.\n",
    "      \n",
    "      \n",
    "3. In reality, nearly all cases will be like above two cases, there is no **perfect solution**, but how do we find a **best solution** to minimise the errors? Just like in 'multivariable linear regression' we are trying to find out the fiting equation to minisize the $\\sum\\limits_{i=1}^n\\varepsilon_i^2$.\n",
    "  \n",
    "4. Projection\n",
    "  \n",
    "    In above 'Example 1', the best solution is: we leave the $\\vec{b}$'s component which is vertical to $\\vec{a}$ alone, only consider its component which has same direction with $\\vec{a}$ ($\\vec{b}$'s vertical projection on $\\vec{a}$), that is: $\\vec{p} = \\theta^*\\vec{a}$, indicated in below figure:\n",
    "    <img src=\"normal_equation_2.jpg\">\n",
    "      \n",
    "    Then the original problem $\\theta\\vec{a} = \\vec{b}$ is converted to find a $\\theta^*$ to make $\\theta^*\\vec{a} = \\vec{p}$ ( **$\\theta^*$ is the best estimator of $\\theta$** ).\n",
    "      \n",
    "    Since $\\vec{e} \\perp \\vec{a}$, then:\n",
    "      \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "      &\\vec{a} \\bullet (\\vec{b} - \\vec{p}) = 0 \\\\\n",
    "      &\\Rightarrow \\vec{a} \\bullet (\\vec{b} - \\theta^*\\vec{a}) = 0 \\\\\n",
    "      &\\Rightarrow \\vec{a}^T \\bullet (\\vec{b} - \\theta^*\\vec{a}) = 0 \\text{ (use }\\vec{a}^T \\text{so it can be better extended to a higher dimensional matrix)} \\\\\n",
    "      &\\Rightarrow \\vec{a}^T \\bullet \\vec{b} = \\theta^*\\vec{a}^T \\bullet \\vec{a} \\\\\n",
    "      &\\Rightarrow \\theta^* = \\frac{\\vec{a}^T \\bullet \\vec{b}}{\\vec{a}^T \\bullet \\vec{a}}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "      \n",
    "    Lets see for above 'Example 2' how do we extend the theory we just got:\n",
    "      \n",
    "    In above 'Example 2', the best solution is: we leave $\\vec{b}$'s component which is vertical to plane P alone, only consider its component which is inside plane P ( $\\vec{b}$'s vertical projection on plane P ), that is:\n",
    "      \n",
    "    $$\n",
    "    \\vec{p} = \\theta^*\\begin{pmatrix}\\vec{a_1} & \\vec{a_2}\\end{pmatrix}, \\, \\\n",
    "    \\theta^* = \\\n",
    "    \\begin{pmatrix}\n",
    "      \\theta_1^* \\\\\n",
    "      \\theta_2^*\n",
    "    \\end{pmatrix}\n",
    "    \\quad\n",
    "    (\\, \\text{that is: }\\vec{p} = \\theta_1^*\\vec{a_1} + \\theta_2^*\\vec{a_2} \\,)\n",
    "    $$\n",
    "      \n",
    "    indicated in below figure:\n",
    "    <img src=\"normal_equation_3.jpg\">\n",
    "      \n",
    "    Then the original problem $\\theta_1\\vec{a_1} + \\theta_2\\vec{a_2} = \\vec{b}$ is converted to find a $\\theta^* = \\begin{pmatrix}\\theta_1^* \\\\ \\theta_2^*\\end{pmatrix}$ to make $\\vec{p} = \\theta^*\\begin{pmatrix}\\vec{a_1} & \\vec{a_2}\\end{pmatrix}$ ( **$\\theta^*$ is the best estimator of $\\theta$** ).\n",
    "      \n",
    "    ***\n",
    "    ***\n",
    "    <center>Lets verify some very basic stuff</center>\n",
    "      \n",
    "    Lets say in $R^3$ space we have three basis vectors $\\vec{a_1} = (a_{1x}, a_{1y}, a_{1z})$, $\\vec{a_2} = (a_{2x}, a_{2y}, a_{2z})$ and $\\vec{a_3} = (a_{3x}, a_{3y}, a_{3z})$, and a combination of constants $\\theta_1$, $\\theta_2$ and $\\theta_3$ to make: $\\vec{p} = \\theta_1\\vec{a_1} + \\theta_2\\vec{a_2} + \\theta_3\\vec{a_3}$.\n",
    "      \n",
    "    Most straightforward calculation: \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "      \\vec{p} &= \\theta_1\\vec{a_1} + \\theta_2\\vec{a_2} + \\theta_3\\vec{a_3} \\\\\n",
    "      &= \\theta_1(a_{1x}, a_{1y}, a_{1z}) + \\theta_2(a_{2x}, a_{2y}, a_{2z}) + \\theta_3(a_{3x}, a_{3y}, a_{3z}) \\\\\n",
    "      &= (\\theta_1a_{1x} + \\theta_2a_{2x} + \\theta_3a_{3x}, \\theta_1a_{1y} + \\theta_2a_{2y} + \\theta_3a_{3y}, \\theta_1a_{1z} + \\theta_2a_{2z} + \\theta_3a_{3z})\n",
    "    \\end{align*}\n",
    "    $$\n",
    "      \n",
    "    Matrix way:\n",
    "    $$\n",
    "    \\text{Let matrix }A = \\\n",
    "    \\begin{pmatrix}\n",
    "      | & | & | \\\\\n",
    "      \\vec{a_1} & \\vec{a_2} & \\vec{a_3} \\\\\n",
    "      | & | & |\n",
    "    \\end{pmatrix}\n",
    "    = \\begin{pmatrix}\n",
    "      a_{1x} & a_{2x} & a_{3x} \\\\\n",
    "      a_{1y} & a_{2y} & a_{3y} \\\\\n",
    "      a_{1z} & a_{2z} & a_{3z}\n",
    "    \\end{pmatrix}, \\,\n",
    "    \\text{and matrix }\\theta = \\\n",
    "    \\begin{pmatrix}\n",
    "      \\theta_1 \\\\\n",
    "      \\theta_2 \\\\\n",
    "      \\theta_3\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "      \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "      \\text{then }\\vec{p} &= A\\theta \\\\\n",
    "      &= (\\theta_1a_{1x} + \\theta_2a_{2x} + \\theta_3a_{3x}, \\theta_1a_{1y} + \\theta_2a_{2y} + \\theta_3a_{3y}, \\theta_1a_{1z} + \\theta_2a_{2z} + \\theta_3a_{3z})\n",
    "    \\end{align*}\n",
    "    $$\n",
    "      \n",
    "    ***\n",
    "    ***\n",
    "      \n",
    "    Lets continue to extend the theory we got from 'Example 1' $R^2$ to 'Example 2' $R^3$:\n",
    "    \n",
    "    $$\n",
    "    \\text{Let matrix }A = \\begin{pmatrix}| & | \\\\ \\vec{a_1} & \\vec{a_2} \\\\ | & |\\end{pmatrix}, \\, \\\n",
    "    \\text{and matrix}\\theta^* = \\begin{pmatrix}\\theta_1^* \\\\ \\theta_2^*\\end{pmatrix} \\\n",
    "    \\quad \\\n",
    "    (\\text{maybe you already noticed: }|A\\theta^* - \\vec{b}|^2 \\text{ is the }\\sum\\limits_{i=1}^n\\varepsilon_i^2 \\text{ we tried to minisize in the least square method})\n",
    "    $$\n",
    "      \n",
    "    We find the $\\theta\\vec{a} = \\vec{b}$ in $R^2$ now in $R^3$ is extended to $A\\vec{\\theta} = \\vec{b}$;\n",
    "    \n",
    "    And correspondingly the $\\theta^*\\vec{a} = \\vec{p}$ in $R^2$ now in $R^3$ is extended to $A\\vec{\\theta^*} = \\vec{p}$;\n",
    "      \n",
    "    Since $\\vec{e} \\perp \\vec{p}$, then:\n",
    "    \n",
    "    $$\n",
    "    \\begin{cases}\n",
    "      a_1^T(\\vec{b} - A\\vec{\\theta^*}) = 0 \\\\\n",
    "      a_2^T(\\vec{b} - A\\vec{\\theta^*}) = 0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "      &\\Rightarrow \\begin{pmatrix}- a_1^T - \\\\ - a_2^T -\\end{pmatrix}(\\vec{b} - A\\vec{\\theta^*}) = 0 \\\\\n",
    "      &\\Rightarrow A^T(\\vec{b} - A\\vec{\\theta^*}) = 0 \\\\\n",
    "      &\\text{ (remember in }R^2 \\text{ we had }\\vec{a}^T(\\vec{b} - \\theta^*\\vec{a}) = 0, \\text{ it is just a special case of current one: in }R^2 \\text{ we treat }\\vec{a} \\text{ as a matrix which only has one column)} \\\\\n",
    "      &\\Rightarrow A^T\\vec{b} = A^TA\\vec{\\theta^*} \\\\\n",
    "      &\\Rightarrow \\vec{\\theta^*} = (A^TA)^{-1}A^T\\vec{b}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    \n",
    "    We get the result just like we revealed previously!\n",
    "    \n",
    "    And since we cannot guarantee the matrix A is always square matrix, so we cannot always simplify the result to $A^{-1}\\vec{b}$.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit to [掰开揉碎推导Normal Equation](https://zhuanlan.zhihu.com/p/22757336)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
