{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial logistic regression retrospect\n",
    "\n",
    "In previous post [Logistic regression (binomial regression) and regularization](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#Modeling) we revealed the model for logistic regression directly: $h_\\theta(x) = \\frac{1}{1+e^{-\\theta x}}$, for why the model looks like that we already had one explanation in the post: [GLM and exponential family distributions](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm008_GLM_and_exponential_family_distributions/GLM_and_exponential_family_distributions.html#With-above-three-hypotheses,-GLM-$\\Rightarrow$-logistic-regression), in this post lets interpret it in another way.\n",
    "\n",
    "Logistic regression is inspired from linear regression: $h_\\theta(x) = \\theta x$, but to a binary classifier(binomial logistic regression) we hope the corresponding $\\theta x$ part can indicate a probability: the probability ($p$) of the sample point belongs to class $A$ (then for $\\bar{A} \\text{ is } 1-p$), since $p$ is a probability, then its range should be $[0,1]$, but the reality is $\\theta x$ can take any value, for achieving what we want we can introduce in [odds](https://en.wikipedia.org/wiki/Odds):\n",
    "\n",
    "$$\n",
    "\\text{odds } = \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "and log-**it** (it â†’ odds, log-__odds__):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  ln\\frac{p}{1-p} &= \\theta x \\\\\n",
    "  \\Rightarrow p &= \\frac{e^{\\theta x}}{1 + e^{\\theta x}} = \\frac{1}{1 + e^{-\\theta x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "now $p \\in [0, 1]$, that is: when we do the $\\frac{1}{1+e^{-\\theta x}}$ transformation to $x$ we get probabilities, and then we can use the odds/log-odds, that is why the model looks like that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend binomial logistic regression to multinomial logistic regression\n",
    "\n",
    "For binomial logistic regression we only have two classes: $A \\text{ and } \\bar{A}$, then we can use the log-odds $ln\\frac{p}{1-p}$ as the binomial classifier indicator: $> 0 \\text{ belongs to class A, } < 0 \\text{ belongs to class } \\bar{A}$ but how do we deal with the case that we have more than two classes, how do we extend the log-odds indicator?\n",
    "\n",
    "Lets say the sample sapce can be classified into $k$ classes, for solving above question, we can go with the below approach:\n",
    "\n",
    "We construct amount $k$ classifiers, each of these $k$ classifiers just does exactly same stuff like the previous [binomial logistic regression](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#How-to-estimate-the-$\\theta$:-MLE-(Maximum-Likelihood-Estimation)) does: evaluate each of the training sample point between class $k_i$ and $\\bar{k_i}$, that is evaluating the $ln\\frac{p_{k_i}}{p_{\\bar{k_i}}}$.\n",
    "\n",
    "Lets say we have 5 samples: $x_0, x_1, x_2, x_3, x_4$, and they can be classified into 3 classes: $k_0, k_1, k_2$, and lets define $p_{k_i}^{x_j}$ represents the probability of that sample $x_j$ belongs to class $k_i$, then after running through this approach eventually we will get:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  p_{k_0}^{x_0} & p_{k_1}^{x_0} & p_{k_2}^{x_0} \\\\\n",
    "  p_{k_0}^{x_1} & p_{k_1}^{x_1} & p_{k_2}^{x_1} \\\\\n",
    "  p_{k_0}^{x_2} & p_{k_1}^{x_2} & p_{k_2}^{x_2} \\\\\n",
    "  p_{k_0}^{x_3} & p_{k_1}^{x_3} & p_{k_2}^{x_3} \\\\\n",
    "  p_{k_0}^{x_4} & p_{k_1}^{x_4} & p_{k_2}^{x_4}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "At last, we just pick up the class has the most probability the input belongs to as our predicition result (from each row)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-written digits recognition with multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sets the backend of matplotlib to the 'inline' backend.\n",
    "#\n",
    "# With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook,\n",
    "# directly below the code cell that produced it.\n",
    "# The resulting plots will then also be stored in the notebook document.\n",
    "#\n",
    "# More details: https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat(os.getcwd() + '/hand_written_digits.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X'].shape, data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 1))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.insert(data['X'], 0, values=np.ones(data['X'].shape[0]), axis=1)\n",
    "y = data['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data contains amount 5000 of hand-written digits, and each single digit holds a 20 by 20 pixels grid, that is each row of above $X$ represents one digit, and each of its component is a float number which represents the grayscale intensity of one of the 20*20 pixels.\n",
    "\n",
    "And we also noticed that the value of $y \\in \\{1,2,3,4,5,6,7,8,9,10\\}$, so the value of $y$ is not the actual number for that corresponding row of $X$, but a class label: class 1, class 2, ... , class 10.\n",
    "\n",
    "Partial example of the data:\n",
    "\n",
    "<img src=\"./hand_written_digits.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cost_reg` is exactly copied over from previous post: [New cost function with regularization item](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#New-cost-function-with-regularization-item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_reg(theta, X, y, alpha):\n",
    "    theta = np.reshape(theta, (-1, len(theta)))\n",
    "\n",
    "    assert X.shape[1] == theta.shape[1], \\\n",
    "      'Improper shape of theta, expected to be: {}, actual: {}'.format((1, X.shape[1]), theta.shape)\n",
    "\n",
    "    part0 = np.multiply(y, np.log(sigmoid(X @ theta.T)))\n",
    "    part1 = np.multiply(1 - y, np.log(1 - sigmoid(X @ theta.T)))\n",
    "    reg = alpha / (2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))\n",
    "\n",
    "    return -np.sum(part0 + part1) / len(X) + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gradient_reg` exactly does what the previous [gradient_reg](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#New-gradient-function-with-regularization-item) does, just we replace the previous for loop calculating way with current pure matrix operating way (then reset grad[0,0] afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_reg(theta, X, y, alpha):\n",
    "    theta = np.reshape(theta, (-1, len(theta)))\n",
    "\n",
    "    assert X.shape[1] == theta.shape[1], \\\n",
    "      'Improper shape of theta, expected to be: {}, actual: {}'.format((1, X.shape[1]), theta.shape)\n",
    "\n",
    "    error = sigmoid(X @ theta.T) - y\n",
    "    \n",
    "    grad = ((X.T @ error) / len(X)).T + alpha / len(X) * theta\n",
    "    \n",
    "    # Reset grad[0,0] to make the intercept gradient is not regularized.\n",
    "    grad[0, 0] = np.sum(np.multiply(error, X[:, [0]])) / len(X)\n",
    "\n",
    "    return grad.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate each of the training sample point between class $k_i$ and $\\bar{k_i}$, that is evaluating the $ln\\frac{p_{k_i}}{p_{\\bar{k_i}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def one_vs_all(X, y, num_labels, alpha):\n",
    "    # 'k by (n+1), n=20*20 here' array for the parameters of each of the k classifiers.\n",
    "    all_theta = np.zeros((num_labels, X.shape[1]))\n",
    "    \n",
    "    # Labels are 1-indexed instead of 0-indexed, it is decided by the data `y`.\n",
    "    for i in range(1, num_labels + 1):\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Set the components of y which are class `i` as 1, all others as 0.\n",
    "        y_i = np.array([1 if label == i else 0 for label in y])\n",
    "        y_i = np.reshape(y_i, (X.shape[0], 1))\n",
    "        \n",
    "        # Minimize the objective function.\n",
    "        fmin = minimize(fun=cost_reg, x0=theta, args=(X, y_i, alpha), method='TNC', jac=gradient_reg)\n",
    "        \n",
    "        all_theta[i-1, :] = fmin.x\n",
    "        \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 401)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta = one_vs_all(X, y, 10, 1)\n",
    "all_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(X, all_theta):\n",
    "    # Compute the class probability for each class for each training instance.\n",
    "    h = sigmoid(X @ all_theta.T)\n",
    "    \n",
    "    # Generate array of the index with the maximum probability for each training instance.\n",
    "    h_argmax = np.argmax(h, axis=1)\n",
    "    \n",
    "    # Because here the array is zero-indexed, and our class labels are 1-indexed,\n",
    "    # we need to add one to get the real label prediction.\n",
    "    return h_argmax + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 94.46%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_all(X, all_theta)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = sum(map(int, correct)) / float(len(correct))\n",
    "print('Total accuracy: {0:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [æœºå™¨å­¦ä¹ ç»ƒä¹ ï¼ˆå››ï¼‰â€”â€”å¤šå…ƒé€»è¾‘å›žå½’](https://blog.csdn.net/and_w/article/details/53260460)\n",
    "\n",
    "- [Multinomial Logistic Regression](https://blog.csdn.net/baimafujinji/article/details/51703322)\n",
    "\n",
    "- [Machine Learning Exercise 3 - Multi-Class Classification](https://github.com/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ML-Exercise3.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
