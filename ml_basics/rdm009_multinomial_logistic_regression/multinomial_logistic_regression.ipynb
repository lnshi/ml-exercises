{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial logistic regression retrospect\n",
    "\n",
    "In previous post [Logistic regression (binomial regression) and regularization](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#Modeling) we revealed the model for logistic regression directly: $h_\\theta(x) = \\frac{1}{1+e^{-\\theta x}}$, for why the model looks like that we already had one explanation in the post: [GLM and exponential family distributions](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm008_GLM_and_exponential_family_distributions/GLM_and_exponential_family_distributions.html#With-above-three-hypotheses,-GLM-$\\Rightarrow$-logistic-regression), in this post lets interpret it in another way.\n",
    "\n",
    "Logistic regression is inspired from linear regression: $h_\\theta(x) = \\theta x$, but to a binary classifier(binomial logistic regression) we hope the corresponding $\\theta x$ part can indicate a probability: the probability ($p$) of the sample point belongs to class $A$ (then for $\\bar{A} \\text{ is } 1-p$), since $p$ is a probability, then its range should be $[0,1]$, but the reality is $\\theta x$ can take any value, for achieving what we want we can introduce in [odds](https://en.wikipedia.org/wiki/Odds):\n",
    "\n",
    "$$\n",
    "\\text{odds } = \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "and log-**it** (it â†’ odds, log-__odds__):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  ln\\frac{p}{1-p} &= \\theta x \\\\\n",
    "  \\Rightarrow p &= \\frac{e^{\\theta x}}{1 + e^{\\theta x}} = \\frac{1}{1 + e^{-\\theta x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "now $p \\in [0, 1]$, that is: when we do the $\\frac{1}{1+e^{-\\theta x}}$ transformation to $x$ we get probabilities, and then we can use the odds/log-odds, that is why the model looks like that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend binomial logistic regression to multinomial logistic regression\n",
    "\n",
    "For binomial logistic regression we only have two classes: $A \\text{ and } \\bar{A}$, then we can use the log-odds $ln\\frac{p}{1-p}$ as the binomial classifier indicator: $> 0 \\text{ belongs to class A, } < 0 \\text{ belongs to class } \\bar{A}$ but how do we deal with the case that we have more than two classes, how do we extend the log-odds indicator?\n",
    "\n",
    "Lets say the sample sapce can be distributed into $k$ classes, and the last class $k$ has special meaning: any sample point not belongs to any of the first $k-1$ classes falls into class $k$. For answering above question, we can choose one class from the $k$ classes as the baseline class, then we use the log-odds of other classes' probabilities to this baseline class's probability as the multinomial classifier's indicator, usually we pick up the last class $k$ as the baseline class, that is:\n",
    "\n",
    "$$\n",
    "ln\\frac{p_i}{p_k} = \\theta_i x, \\enspace i = 1, 2, \\dots, k-1\n",
    "$$\n",
    "\n",
    "And actually in this post: [GLM and exponential family distributions](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm008_GLM_and_exponential_family_distributions/GLM_and_exponential_family_distributions.html#Reference-Point) we already calculated that:\n",
    "\n",
    "$$\n",
    "p_k = 1 \\Big/ \\sum\\limits_{i=1}^k exp(\\eta_i) = 1 \\Big/ \\sum\\limits_{i=1}^k exp(\\theta_i^Tx)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-written digits recognition with multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sets the backend of matplotlib to the 'inline' backend.\n",
    "#\n",
    "# With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook,\n",
    "# directly below the code cell that produced it.\n",
    "# The resulting plots will then also be stored in the notebook document.\n",
    "#\n",
    "# More details: https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat(os.getcwd() + '/hand_written_digits.mat')  \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X'].shape, data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.insert(data['X'], 0, values=np.ones(data['X'].shape[0]), axis=1)\n",
    "y = data['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data contains amount 5000 of hand-written digits, and each single digit holds a 20 by 20 pixels grid, that is each row of above $X$ represents one digit, and each of its component is a float number which represents the grayscale intensity of one of the 20*20 pixels, partial example of the data:\n",
    "\n",
    "<img src=\"./hand_written_digits.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cost_reg` is exactly copied over from previous post: [New cost function with regularization item](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#New-cost-function-with-regularization-item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_reg(theta, X, y, alpha):\n",
    "    theta = np.reshape(theta, (-1, len(theta)))\n",
    "\n",
    "    assert X.shape[1] == theta.shape[1], \\\n",
    "      'Improper shape of theta, expected to be: {}, actual: {}'.format((1, X.shape[1]), theta.shape)\n",
    "\n",
    "    part0 = np.multiply(y, np.log(sigmoid(X @ theta.T)))\n",
    "    part1 = np.multiply(1 - y, np.log(1 - sigmoid(X @ theta.T)))\n",
    "    reg = alpha / (2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))\n",
    "\n",
    "    return -np.sum(part0 + part1) / len(X) + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_reg(theta, X, y, alpha):\n",
    "    theta = np.reshape(theta, (-1, len(theta)))\n",
    "\n",
    "    assert X.shape[1] == theta.shape[1], \\\n",
    "      'Improper shape of theta, expected to be: {}, actual: {}'.format((1, X.shape[1]), theta.shape)\n",
    "\n",
    "    error = sigmoid(X @ theta.T) - y\n",
    "    grad = \n",
    "    \n",
    "    \n",
    "    grad = np.zeros(parameters)\n",
    "    \n",
    "\n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:, [i]])\n",
    "\n",
    "        if i == 0:\n",
    "            # No penalization to theta_0.\n",
    "            grad[i] = np.sum(term) / len(X)\n",
    "        else:\n",
    "            grad[i] = np.sum(term) / len(X) + alpha / len(X) * theta[:, i]\n",
    "\n",
    "    return grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
