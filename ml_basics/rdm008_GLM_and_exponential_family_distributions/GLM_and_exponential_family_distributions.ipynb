{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First\n",
    "\n",
    "[Linear regression](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm001_multivariable_linear_regression_gradient_descent/multivariable_linear_regression_gradient_descent.html), [logistic regression](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html) and some others linear models like softmax regression etc. are all just specialized forms of [GLM (Generalized Linear Model)](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function).\n",
    "\n",
    "Usually when we build one model, we always firstly go to analyze which distribution does the target sample space subject to, for GLM we should look into [Exponential family](https://en.wikipedia.org/wiki/Exponential_family), then we convert that distribution's parameters to GLM's parameters, then we can proceed our solution with that specialized form of GLM, that is:\n",
    "\n",
    "<img src=\"./modeling.svg\">\n",
    "\n",
    "In [linear regression](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm001_multivariable_linear_regression_gradient_descent/multivariable_linear_regression_gradient_descent.html) we assume: $y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$, and in [logistic regression](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#How-to-estimate-the-$\\theta$:-MLE-(Maximum-Likelihood-Estimation)) we assume: $y|x;\\theta \\sim Bernoulli(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential family\n",
    "\n",
    "### [Members of exponential family distributions](https://en.wikipedia.org/wiki/Exponential_family#Examples_of_exponential_family_distributions)\n",
    "\n",
    "### Exponential family distributions can be expressed in a generic form as below:\n",
    "\n",
    "$$\n",
    "  f(y|x; \\eta) = h(y) exp\\big(\\eta(\\theta) \\cdot T(y) - A(\\eta)\\big)\n",
    "$$\n",
    "\n",
    "- $\\eta = \\eta(\\theta)$ is its natural parameter\n",
    "- $T(y)$ is the [sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic) (usually $T(y) = y$)\n",
    "- $A(\\eta)$ is the log-[partition function](https://en.wikipedia.org/wiki/Partition_function_(mathematics)) ($A(\\eta)$ plays the regularization role, to make: $\\sum f\\big(T(y); \\eta\\big) = 1$)\n",
    "\n",
    "That is: $h(y)$, $T(y)$ and $A(\\eta)$ determined a new distribution, the transformed parameter $\\eta = \\eta(\\theta)$ is this distribution's parameter.\n",
    "\n",
    "### Bernoulli distribution in GLM form\n",
    "\n",
    "The probability mass function $f$ of bernoulli distribution over possible outcomes $x$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f(x;p) &= p^x(1-p)^{1-x} \\\\\n",
    "  &= exp\\big(xlnp + (1-x)ln(1-p)\\big) \\\\\n",
    "  &= exp\\big(xln\\frac{p}{1-p} + ln(1-p)\\big) \\enspace \\text{for } x \\in \\{0, 1\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta = ln\\frac{p}{1-p} \\Rightarrow e^\\eta = \\frac{p}{1-p} \\Rightarrow p = \\frac{1}{1 + e^{-\\eta}}\n",
    "$$\n",
    "\n",
    "that is, when:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  h(x) &= 1 \\\\\n",
    "  T(x) &= x \\\\\n",
    "  A(\\eta) &= -ln(1-p) = ln(1+e^\\eta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "GLM subjects bernoulli distribution.\n",
    "\n",
    "### Gaussian distribution (normal distribution) in GLM form\n",
    "\n",
    "The probability density function $f$ of normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f(x| \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) \\\\\n",
    "  &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}x^2)exp(\\mu x - \\frac{\\mu^2}{2})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta = \\mu\n",
    "$$\n",
    "\n",
    "that is, when:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  h(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}x^2) \\\\\n",
    "  T(x) &= x \\\\\n",
    "  A(\\eta) &= \\frac{\\mu^2}{2} = \\frac{\\eta^2}{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "GLM subjects normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with GLM\n",
    "\n",
    "When modeling with GLM we need to comply with the below three hypotheses：\n",
    "\n",
    "1. $y|x;\\theta \\sim ExponentialFamily(\\eta)$, that is: the conditional probability of $y$ given $x$ subjects to exponential family distributions;\n",
    "\n",
    "2. **The target of modeling with GLM** is finding out the fitting function $h(x)$ to make $h(x) = E\\big[T(y)|x;\\theta\\big]$, but due to in most cases $T(y) = y$, so the target changes to find out the fitting function $h(x)$ to make $h(x) = E\\big[y|x;\\theta\\big]$;\n",
    "\n",
    "3. The natural parameter $\\eta$ should be linear with $x$;\n",
    "\n",
    "### With above three hypotheses, GLM $\\Rightarrow$ linear regression\n",
    "\n",
    "- With above hypothesis 1:\n",
    "\n",
    "$$\n",
    "y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2) \n",
    "$$\n",
    "\n",
    "\n",
    "- With above hypothesis 2, and check the table [normal distribution known variance](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions) for the 'Natural parameter(s) $\\eta$', 'Sufficient statistic $T(x)$', and 'Inverse parameter mapping' we can conclude out fitting function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f_\\theta(x) &= E\\big[y|x;\\theta\\big] = \\mu \\\\\n",
    "  &= \\sigma\\eta \\enspace \\text{(Inverse parameter mapping)} \\\\\n",
    "  &\\text{refer to above 'Bernoulli distribution in GLM form' section to see how to calculate 'Inverse parameter mapping'} \\\\\n",
    "  &\\text{and since variance here doesn't affect the accuracy of our model, we make: } \\sigma = 1 \\\\\n",
    "  \\Rightarrow &f_\\theta(x) = \\eta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "- With above hypothesis 3:\n",
    "\n",
    "$$\n",
    "\\eta = \\theta^Tx\n",
    "$$\n",
    "\n",
    "Then finally for linear regresion we can conclude the fitting equation is: $f_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1 + \\dots + \\theta_mx_m$, as we declared in this previous post [Multivariable linear regression(gradient descent)](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm001_multivariable_linear_regression_gradient_descent/multivariable_linear_regression_gradient_descent.html#And-we-want-to-find-out-the-fitting-equation:).\n",
    "\n",
    "### With above three hypotheses, GLM $\\Rightarrow$ logistic regression\n",
    "\n",
    "- With above hypothesis 1:\n",
    "\n",
    "$$\n",
    "y|x;\\theta \\sim Bernoulli(p)\n",
    "$$\n",
    "\n",
    "\n",
    "- With above hypothesis 2, and check the table [Bernoulli distribution](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions) for the 'Natural parameter(s) $\\eta$', 'Sufficient statistic $T(x)$', and 'Inverse parameter mapping' we can conclude out fitting function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f_\\theta(x) &= E\\big[y|x;\\theta\\big] = p \\\\\n",
    "  &= \\frac{1}{1 + e^{-\\eta}} \\enspace \\text{(Inverse parameter mapping)} \\\\\n",
    "  &\\text{refer to above 'Bernoulli distribution in GLM form' section to see how to calculate 'Inverse parameter mapping'}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "- With above hypothesis 3:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\eta &= \\theta^Tx \\\\\n",
    "  \\Rightarrow f_\\theta(x) &= \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then finally for logistic regresion we can conclude the fitting equation is: $f_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}$, as we declared in this previous post [Logistic regression (binomial regression) and regularization](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#Modeling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression ([categorical distribution (variant 3)](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions)) in GLM form\n",
    "\n",
    "Lets say $y$ can be classified into $k$ classes: $y \\in \\{1, 2, \\dots, k\\}$, the probabilities are $p_1, p_2, \\dots, p_k$ respectively, and the last class $k$ has special meaning: any sample point not belongs to any of the first $k-1$ classes falls into class $k$, that is:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "  p_i \\quad i \\in \\{1, 2, \\dots, k-1\\} \\\\\n",
    "  p_k = 1 - \\sum\\limits_{i=1}^{k-1}p_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "lets define $T(y) \\in R^{k-1}$:\n",
    "\n",
    "$$\n",
    "T(1) = \\begin{pmatrix}1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}, \\\n",
    "T(2) = \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}, \\\n",
    "T(3) = \\begin{pmatrix}0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0\\end{pmatrix}, \\\n",
    "\\dots, \\\n",
    "T(k-1) = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1\\end{pmatrix}, \\\n",
    "T(k) = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "for expressing above $T(y) \\enspace y \\in {1,2,\\dots,k}$ function better, lets introduce in [indicator function](https://en.wikipedia.org/wiki/Indicator_function) $1\\{\\cdot\\}$: $1\\{true\\} = 1$ and $1\\{false\\} = 0$\n",
    "\n",
    "that is $T(y)|x; p \\sim CategoricalDistribution(x_1,x_2,\\dots,x_k;1;p_1,p_2,\\dots,p_k)$, thus: $E\\big[T(y)|x; p\\big] = 1\\cdot p_i = p_i$ (mean of [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) or also can refer to mean of the [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution)).\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "#### Why the PMF has no coefficient? \n",
    "\n",
    "[categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) can be treated as a special form of the [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) with $k>2$ and $n=1$, and since $n=1$, then the coefficient $\\frac{n!}{x_1! x_2! \\dots x_k!}$ of the multinomial distribution's PMF(https://en.wikipedia.org/wiki/Multinomial_distribution#Probability_mass_function) will be all 1, that is why for our below 'PMF of $T(y)|x; p$' has the form: $p_1^{x_1} p_2^{x_2} \\dots p_k^{x_k}$ only, **WITHOUT** that coefficient, more details:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "  \\text{class 1 is chosen: } \\frac{1!}{1! \\cdot 0! \\cdot 0! \\dots 0!} = 1 \\\\\n",
    "  \\text{class 2 is chosen: } \\frac{1!}{0! \\cdot 1! \\cdot 0! \\dots 0!} = 1 \\\\\n",
    "  \\text{class 3 is chosen: } \\frac{1!}{0! \\cdot 0! \\cdot 1! \\dots 0!} = 1 \\\\\n",
    "  \\vdots\n",
    "\\end{cases}\n",
    "$$\n",
    "***\n",
    "***\n",
    "\n",
    "#### Reference Point\n",
    "PMF of $T(y)|x; p$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f\\big(T(y)|x;p\\big) &= p_1^{1\\{y=1\\}} p_2^{1\\{y=2\\}} \\dots p_{k-1}^{1\\{y=k-1\\}} p_k^{1\\{y=k\\}} \\\\\n",
    "  &= p_1^{1\\{y=1\\}} p_2^{1\\{y=2\\}} \\dots p_{k-1}^{1\\{y=k-1\\}} p_k^{1 - \\sum\\limits_{i=1}^{k-1}1\\{y=i\\}} \\\\\n",
    "  &\\text{note: above } 1 - \\sum\\limits_{i=1}^{k-1}1\\{y=i\\} \\text{ the 1 actually is a vector with all 1 components} \\\\\n",
    "  &= p_1^{T_1(y)} p_2^{T_2(y)} \\dots p_{k-1}^{T_{k-1}(y)} p_k^{1 - \\sum\\limits_{i=1}^{k-1}T_i(y)} \\\\\n",
    "  &=  exp\\Big( T_1(y)lnp_1 + T_2(y)lnp_2 + \\dots + T_{k-1}(y)lnp_{k-1} + \\big(1 - \\sum\\limits_{i=1}^{k-1}T_i(y)\\big)lnp_k \\Big) \\\\\n",
    "  &= exp\\Big( T_1(y)ln\\frac{p_1}{p_k} + T_2(y)ln\\frac{p_2}{p_k} + \\dots + T_{k-1}(y)ln\\frac{p_{k-1}}{p_k} + lnp_k \\Big) \\\\\n",
    "  &= h(y) exp\\big( \\eta^T T(y) - A(\\eta) \\big)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\eta &= \\begin{pmatrix} ln\\frac{p_1}{p_k} \\\\ ln\\frac{p_2}{p_k} \\\\ \\vdots \\\\ ln\\frac{p_{k-1}}{p_k} \\end{pmatrix} \\\n",
    "  = \\begin{pmatrix} \\theta_1^Tx \\\\ \\theta_2^Tx \\\\ \\vdots \\\\ \\theta_{k-1}^Tx \\end{pmatrix} \\\\\n",
    "  A(\\eta) &= -lnp_k \\\\\n",
    "  h(x) &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "calculate $p_i$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\eta_i &= ln\\frac{p_i}{p_k} \\\\\n",
    "  &\\Rightarrow exp(\\eta_i) = \\frac{p_i}{p_k} \\\\\n",
    "  &\\Rightarrow p_kexp(\\eta_i) = p_i \\\\\n",
    "  &\\because \\sum\\limits_{i=1}^k p_i = 1 = \\sum\\limits_{i=1}^k p_k exp(\\eta_i) = p_k \\sum\\limits_{i=1}^k exp(\\eta_i) \\\\\n",
    "  &\\therefore p_k = 1 \\Big/ \\sum\\limits_{i=1}^k exp(\\eta_i) = 1 \\Big/ \\sum\\limits_{i=1}^k exp(\\theta_i^Tx) \\\\\n",
    "  \\Rightarrow p_i &= p_k exp(\\eta_i) = \\frac{exp(\\eta_i)}{\\sum\\limits_{i=1}^k exp(\\eta_i)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "that is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  p(y=i|x;\\theta) &= p_i \\\\\n",
    "  &= \\frac{exp(\\eta_i)}{\\sum\\limits_{j=1}^k exp(\\eta_j)} \\\\\n",
    "  &= \\frac{exp(\\theta_i^T x)}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "With above hypothesis 2 lets define the fitting equation for $T(y)|x; \\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f_\\theta(x) &= E\\Big[ T(y)|x; \\theta \\Big] \\\\\n",
    "  &= E \\left[ \\\n",
    "    \\begin{array}{c|c}\n",
    "      1\\{y=1\\} \\\\\n",
    "      1\\{y=2\\} \\\\\n",
    "      \\vdots & x; \\theta \\\\\n",
    "      1\\{y=k-2\\} \\\\\n",
    "      1\\{y=k-1\\}\n",
    "    \\end{array}\n",
    "    \\right] \\\\\n",
    "  &= \\begin{bmatrix} p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_{k-2} \\\\ p_{k-1} \\end{bmatrix} \\\\\n",
    "  &= \\begin{bmatrix} \\\n",
    "        \\frac{exp(\\theta_1^T x)}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x)} \\\\\n",
    "        \\frac{exp(\\theta_2^T x)}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{exp(\\theta_{k-2}^T x)}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x)} \\\\\n",
    "        \\frac{exp(\\theta_{k-1}^T x)}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x)}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "PMF of $T(y)|x; \\theta$\n",
    "\n",
    "$$\n",
    "  f\\big( T(y)|x; \\theta \\big) = \\prod_{i=1}^k \\Bigg( \\frac{exp(\\theta_i^T x)}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x)} \\Bigg)^{1\\{y=i\\}}\n",
    "$$\n",
    "\n",
    "***\n",
    "***\n",
    "with sample dataset:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\\n",
    "(x_1^{(1)}, x_2^{(1)}, \\dots, x_m^{(1)}, y^{(1)}),\n",
    "(x_1^{(2)}, x_2^{(2)}, \\dots, x_m^{(2)}, y^{(2)}),\n",
    "\\dots,\n",
    "(x_1^{(n)}, x_2^{(n)}, \\dots, x_m^{(n)}, y^{(n)})\n",
    "\\\\\n",
    "& \\\n",
    "x_i^{(j)} \\\n",
    "\\Big(\n",
    "  \\begin{aligned}\n",
    "    i = 1, 2, \\dots, m \\\\\n",
    "    j = 1, 2, \\dots, n\n",
    "  \\end{aligned}\n",
    "\\Big) \\\n",
    "\\text{represents the value of the feature } x_i \\\n",
    "\\text{of the } j^{th} \\\n",
    "\\text{sample record}\n",
    "\\\\\n",
    "& \\\n",
    "y^{(i)} \\\n",
    "\\big(\n",
    "  \\begin{aligned}\n",
    "    i = 1, 2, \\dots, n\n",
    "  \\end{aligned}\n",
    "\\Big) \\\n",
    "\\text{represents the target value of the } i^{th} \\\n",
    "\\text{sample record}\n",
    "\\end{align*}\n",
    "$$\n",
    "***\n",
    "***\n",
    "\n",
    "**log-it** and apply MLE(consider all the samples) to find the $\\theta$:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\sum\\limits_{c=1}^n log \\prod_{i=1}^k \\Bigg( \\frac{exp(\\theta_i^T x^{(c)})}{\\sum\\limits_{j=1}^k exp(\\theta_j^T x^{(c)})} \\Bigg)^{1\\{y^{(c)}=i\\}}\n",
    "$$\n",
    "\n",
    "Find the $\\theta$ with gradient descent algorithm.\n",
    "\n",
    "We are all done, cheers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [广义线性模型（Generalized Linear Model）](https://zhuanlan.zhihu.com/p/22876460)\n",
    "\n",
    "- [机器学习中的logistic regression的sigmoid函数如何解释？为啥要用它？](https://www.zhihu.com/question/23666587/answer/462453898)\n",
    "\n",
    "- [Generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)\n",
    "\n",
    "- [Exponential family](https://en.wikipedia.org/wiki/Exponential_family)\n",
    "\n",
    "- [Sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic)\n",
    "\n",
    "- [Partition function (mathematics)](https://en.wikipedia.org/wiki/Partition_function_(mathematics))\n",
    "\n",
    "- [Canonical form](https://en.wikipedia.org/wiki/Canonical_form)\n",
    "\n",
    "- [Cumulant generating function](https://en.wikipedia.org/wiki/Cumulant_generating_function)\n",
    "\n",
    "- [Multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "\n",
    "- [Categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution)\n",
    "\n",
    "- [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n",
    "\n",
    "- [Joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)\n",
    "\n",
    "- [Indicator function](https://en.wikipedia.org/wiki/Indicator_function)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
