{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the awesome original posts below\n",
    "\n",
    "[AdaBoost Classifier Example In Python](https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464)\n",
    "\n",
    "[Gradient Boosting Decision Tree Algorithm Explained](https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4)\n",
    "\n",
    "[XGBoost Python Example](https://towardsdatascience.com/xgboost-python-example-42777d01001e)\n",
    "\n",
    "## In a nutshell\n",
    "\n",
    "Recall the comparisons between __decision tree__ and __random forest__:\n",
    "\n",
    "> Like decision trees, random forest can be applied to both regression and classification problems.\n",
    ">\n",
    "> As is implied by the names \"Tree\" and \"Forest\", a Random Forest is essentially a collection of Decision Trees. A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results. After a large number of trees are built using this method, each tree votes or chooses the class, and the class receiving the most votes by a simple majority is the \"winner\" or predicted class. There are of course some more detailed differences, but this is the main conceptual difference. (referred from here: Difference between Random Forests and Decision tree)\n",
    ">\n",
    "> The portion of samples that were left out during the construction of each decision tree in the forest are referred to as the Out-Of-Bag (OOB) dataset. As we will see later, the model will automatically evaluate its own performance by running each of the samples in the OOB dataset through the forest.\n",
    ">\n",
    "> Also note that there are laws which demand that the decisions made by models used in issuing loans or insurance be explainable. The latter is known as model interpretability and is one of the reasons why we see random forest models being used heavily in industry.\n",
    "\n",
    "__Random forest__ vs __gradient boosting decision tree__\n",
    "\n",
    "Like random forests, gradient boosting is a set of decision trees. The two main differences are:\n",
    "\n",
    "- How trees are built: random forests builds each tree __independently__ while gradient boosting builds __one tree at a time__, where each new tree helps to correct errors made by __previously trained tree__.\n",
    "    \n",
    "- Combining results: random forests combine results __at the end__ of the process (by averaging or \"majority rules\") while gradient boosting combines results __along the way__.\n",
    "    \n",
    "__gradient boosting__ vs __AdaBoost__\n",
    "\n",
    "- Both AdaBoost and Gradient Boosting build weak learners in a sequential fashion.\n",
    "\n",
    "- Insights from this answer: [Adaboost vs Gradient Boosting](https://datascience.stackexchange.com/a/39201/71969)\n",
    "\n",
    "    * Originally, AdaBoost was designed in such a way that at every step the sample distribution was adapted to __put more weight on misclassified samples__ and __less weight on correctly classified samples__. The final prediction is a weighted average of all the weak learners, where more weight is placed on stronger learners.\n",
    "\n",
    "    * Later, it was discovered that AdaBoost can also be expressed as in terms of the more general framework of additive models with a particular loss function (the exponential loss). See e.g. [chapter 10 in (Hastie) ESL](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)\n",
    "    \n",
    "    * The main differences therefore are that Gradient Boosting is a __generic algorithm__ to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a __special case with a particular loss function__. Hence, gradient boosting is much more flexible.\n",
    "\n",
    "    * Second, AdaBoost can be interpreted from a much more intuitive perspective and can be implemented without the reference to gradients by re-weighting the training samples based on classifications from previous learners.\n",
    "\n",
    "- Insights from this answer: [Intuitive explanations of differences between Gradient Boosting Trees (GBM) & Adaboost](https://stats.stackexchange.com/a/164262/229537)\n",
    "    \n",
    "    * In Gradient Boosting, `shortcomings` (of existing weak learners) are identified by gradients.\n",
    "        \n",
    "    * AdaBoost, `shortcomings` are identified by high-weight data points.\n",
    "    \n",
    "- Others:\n",
    "\n",
    "    * In AdaBoost, the decision trees have a depth of __1__ (i.e. 2 leaves). In addition, the predictions made by each decision tree have varying impact on the final prediction made by the model.\n",
    "    \n",
    "    * However, in gradient boosting, unlike AdaBoost, the Gradient Boost trees have a depth __larger than 1__. In practice, youâ€™ll typically see Gradient Boost being used with a maximum number of leaves of between 8 and 32.\n",
    "    \n",
    "## Finally: XGBoost\n",
    "\n",
    "Insights from this answer: [GBM vs XGBOOST? Key differences?](https://datascience.stackexchange.com/a/16908/71969)\n",
    "\n",
    "> Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.\n",
    ">\n",
    "> We have updated a comprehensive tutorial on introduction to the model, which you might want to take a look at. Introduction to Boosted Trees\n",
    ">\n",
    "> The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost. For model, it might be more suitable to be called as regularized gradient boosting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
