{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the awesome original post here: [Overview of different Optimizers for neural networks](https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3)\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Gradient descent is an iterative machine learning optimization algorithm to reduce the cost function. This will help models to make accurate predictions.\n",
    "\n",
    "Gradient indicates the direction of increase. As we want to find the minimum point in the valley we need to go in the opposite direction of the gradient. We update parameters in the negative gradient direction to minimize the loss.\n",
    "\n",
    "$$\n",
    "    \\theta = \\theta - \\eta \\nabla J(\\theta ; x, y)\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    $\\theta$ is the weight parameter, $\\eta$ is the learning rate and $\\nabla \\mathrm{J}(\\theta ; \\mathrm{x}, \\mathrm{y})$ is the gradient of weight parameter $\\theta$\n",
    "</center>\n",
    "\n",
    "## Types of gradient descent\n",
    "\n",
    "- Batch Gradient Descent or Vanilla Gradient Descent\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- Mini batch Gradient Descent\n",
    "\n",
    "## Role of an optimizer\n",
    "\n",
    "Optimizers update the weight parameters to minimize the loss function. Loss function acts as guides to the terrain telling optimizer if it is moving in the right direction to reach the bottom of the valley, the global minimum.\n",
    "\n",
    "## Types of optimizers\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Momentum is like a ball rolling downhill. The ball will gain momentum as it rolls down the hill.\n",
    "\n",
    "![momentum](./momentum.png)\n",
    "\n",
    "Momentum helps accelerate Gradient Descent(GD) when we have surfaces that curve more steeply in one direction than in another direction. It also dampens the oscillation as shown above.\n",
    "\n",
    "For updating the weights it takes the gradient of the current step as well as the gradient of the previous time steps. This helps us move faster towards convergence.\n",
    "\n",
    "Convergence happens faster when we apply momentum optimizer to surfaces with curves.\n",
    "\n",
    "$$\n",
    "    \\begin{aligned} v_{t} &=\\gamma v_{t-1}+\\eta \\nabla J(\\theta ; x, y) \\\\ \\theta &=\\theta-v_{t} \\end{aligned}\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    Momentum Gradient descent takes gradient of previous time steps into consideration\n",
    "</center>\n",
    "\n",
    "### Nesterov accelerated gradient(NAG)\n",
    "\n",
    "Nesterov acceleration optimization is like a ball rolling down the hill but knows exactly when to slow down before the gradient of the hill increases again.\n",
    "\n",
    "We calculate the gradient not with respect to the current step but with respect to the future step. We evaluate the gradient of the looked ahead and based on the importance then update the weights.\n",
    "\n",
    "![NAG](./nag.png)\n",
    "\n",
    "NAG is like you are going down the hill where we can look ahead in the future. This way we can optimize our descent faster. Works slightly better than standard Momentum.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\theta =\\theta - v_{t} \\\\\n",
    "    &v_{t} = \\gamma v_{t-1} + \\eta \\nabla J\\left(\\theta - \\gamma v_{t-1}\\right) \\\\\n",
    "    &\\theta - \\gamma v_{t-1} \\text{ is the gradient of looked ahead}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Adagrad — Adaptive Gradient Algorithm\n",
    "\n",
    "We need to tune the learning rate in Momentum and NAG which is an expensive process.\n",
    "\n",
    "Adagrad is an adaptive learning rate method. In Adagrad we adopt the learning rate to the parameters. We perform larger updates for infrequent parameters and smaller updates for frequent parameters.\n",
    "\n",
    "It is well suited when we have sparse data as in large scale neural networks. GloVe word embedding uses adagrad where infrequent words required a greater update and frequent words require smaller updates.\n",
    "\n",
    "For SGD, Momentum, and NAG we update for all parameters θ at once. We also use the same learning rate η. In Adagrad we use different learning rate for every parameter θ for every time step t\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\varepsilon}} \\cdot g_{t} \\\\\n",
    "    &G_t \\text{ is sum of the squares of the past gradients w.r.t. to all parameters } \\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Adagrad eliminates the need to manually tune the learning rate.\n",
    "\n",
    "In the denominator, we accumulate the sum of the square of the past gradients. Each term is a positive term so it keeps on growing to make the learning rate η infinitesimally small to the point that algorithm is no longer able learning. Adadelta, RMSProp, and adam tries to resolve Adagrad’s radically diminishing learning rates.\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "- Adadelta is an extension of Adagrad and it also tries to reduce Adagrad’s aggressive, monotonically reducing the learning rate.\n",
    "\n",
    "- It does this by restricting the window of the past accumulated gradient to some fixed size of w. Running average at time t then depends on the previous average and the current gradient.\n",
    "\n",
    "- In Adadelta we do not need to set the default learning rate as we take the ratio of the running average of the previous time steps to the current gradient.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\theta_{t+1} = \\theta_{t} + \\Delta \\theta_{t} \\\\\n",
    "    &\\Delta \\theta = -\\frac{R M S[\\Delta \\theta]_{t-1}}{R M S\\left[g_{t}\\right]} \\cdot g_{t}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "- RMSProp is Root Mean Square Propagation. It was devised by Geoffrey Hinton.\n",
    "\n",
    "- RMSProp tries to resolve Adagrad’s radically diminishing learning rates by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "\n",
    "- In RMSProp learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "RMSProp divides the learning rate by the average of the exponential decay of squared gradients.\n",
    "\n",
    "$$\n",
    "    \\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{(1 - \\gamma) g^{2}_{t-1}} + \\gamma g_{t} + \\varepsilon} \\cdot g_{t}\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    γ is the decay term that takes value from 0 to 1. gt is moving average of squared gradients\n",
    "</center>\n",
    "\n",
    "### Adam — Adaptive Moment Estimation\n",
    "\n",
    "- Another method that calculates the individual adaptive learning rate for each parameter from estimates of first and second moments of the gradients.\n",
    "\n",
    "- It also reduces the radically diminishing learning rates of Adagrad.\n",
    "\n",
    "- Adam can be viewed as a combination of Adagrad, which works well on sparse gradients and RMSprop which works well in online and nonstationary settings.\n",
    "\n",
    "- Adam implements the exponential moving average of the gradients to scale the learning rate instead of a simple average as in Adagrad. It keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "- Adam is computationally efficient and has very little memory requirement.\n",
    "\n",
    "- Adam optimizer is one of the most popular gradient descent optimization algorithms.\n",
    "\n",
    "Adam algorithm first updates the exponential moving averages of the gradient(mt) and the squared gradient(vt) which is the estimates of the first and second moment.\n",
    "\n",
    "Hyper-parameters β1, β2 ∈ [0, 1) control the exponential decay rates of these moving averages as shown below\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &m_{t} = \\beta_{1} m_{t-1} + \\left(1 - \\beta_{1}\\right) g_{t} \\\\\n",
    "    &v_{t} = \\beta_{2} v_{t-1} + \\left(1 - \\beta_{2}\\right) g_{t}^{2} \\\\\n",
    "    &m_{t} \\text{ and } v_{t} \\text{ are estimates of first and second moment respectively}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Moving averages are initialized as 0 leading to moment estimates that are biased around 0 especially during the initial timesteps. This initialization bias can be easily counteracted resulting in bias-corrected estimates\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}} \\\\\n",
    "    &\\hat{v}_{t}=\\frac{V_{t}}{1-\\beta_{2}^{t}} \\\\\n",
    "    &\\hat{m}_{t} \\text{ and } \\hat{v}_{t} \\text{ are bias corrected estimates of first and second moment respectively}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, we update the parameter as shown below\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\frac{\\eta \\widehat{m}_{t}}{\\sqrt{\\hat{v}_{t} + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "### Nadam- Nesterov-accelerated Adaptive Moment Estimation\n",
    "\n",
    "- Nadam combines NAG and Adam.\n",
    "\n",
    "- Nadam is employed for noisy gradients or for gradients with high curvatures.\n",
    "\n",
    "- The learning process is accelerated by summing up the exponential decay of the moving averages for the previous and current gradient.\n",
    "\n",
    "In the diagram below we see can see how different optimizer will converge to the minimum. Adagrad, Adadelta, and RMSprop headed off immediately in the right direction and converge. Momentum and NAG were led off-track, evoking the image of a ball rolling down the hill. NAG corrected itself quickly\n",
    "\n",
    "![performace comparsion](./performance_comparison.gif)\n",
    "\n",
    "## References\n",
    "\n",
    "[Overview of different Optimizers for neural networks](https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3)\n",
    "\n",
    "[Adam: A Method for Stochastic Optimization by Diederik P. Kingma, Jimmy Ba](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "https://arxiv.org/pdf/1609.04747.pdf\n",
    "\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "\n",
    "[优化算法Optimizer比较和总结](https://zhuanlan.zhihu.com/p/55150256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
