{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets assume we have a very simple cost function.\n",
    "def f(x):\n",
    "    return x * x - 2 * x + 1\n",
    "\n",
    "# And its corresponding derivative.\n",
    "def g(x):\n",
    "    return 2 * x - 2\n",
    "\n",
    "# Then we can come out the gradient descent algorithm.\n",
    "def gd(x, alpha, g):\n",
    "    for i in range(20):\n",
    "        grad = g(x)\n",
    "        x -= alpha * grad\n",
    "        \n",
    "        print('[Epoch {}] grad = {}, x = {}'.format(i, grad, x))\n",
    "        \n",
    "        if abs(grad) < 1e-6:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above $f(x)$ we clearly know it reaches its minimum value at $x = 1$, but lets use our 'gd' algorithm to figure the $x$ out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] grad = 8, x = 4.2\n",
      "[Epoch 1] grad = 6.4, x = 3.56\n",
      "[Epoch 2] grad = 5.12, x = 3.048\n",
      "[Epoch 3] grad = 4.096, x = 2.6384\n",
      "[Epoch 4] grad = 3.2767999999999997, x = 2.31072\n",
      "[Epoch 5] grad = 2.6214399999999998, x = 2.0485759999999997\n",
      "[Epoch 6] grad = 2.0971519999999995, x = 1.8388607999999997\n",
      "[Epoch 7] grad = 1.6777215999999995, x = 1.6710886399999998\n",
      "[Epoch 8] grad = 1.3421772799999996, x = 1.536870912\n",
      "[Epoch 9] grad = 1.0737418239999998, x = 1.4294967295999998\n",
      "[Epoch 10] grad = 0.8589934591999997, x = 1.34359738368\n",
      "[Epoch 11] grad = 0.6871947673599998, x = 1.274877906944\n",
      "[Epoch 12] grad = 0.5497558138879999, x = 1.2199023255552\n",
      "[Epoch 13] grad = 0.4398046511103999, x = 1.17592186044416\n",
      "[Epoch 14] grad = 0.35184372088831983, x = 1.1407374883553278\n",
      "[Epoch 15] grad = 0.2814749767106557, x = 1.1125899906842622\n",
      "[Epoch 16] grad = 0.22517998136852446, x = 1.0900719925474098\n",
      "[Epoch 17] grad = 0.18014398509481966, x = 1.0720575940379278\n",
      "[Epoch 18] grad = 0.14411518807585555, x = 1.0576460752303423\n",
      "[Epoch 19] grad = 0.11529215046068453, x = 1.0461168601842739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0461168601842739"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd(5, 0.1, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, the $x$ is gradually approaching 1 from its initial value we set 5.\n",
    "\n",
    "But it took 20 iterations to reach 1.0461168601842739, how about lets increase the learning rate for making it reach there faster, lets set the learning rate 100!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] grad = 8, x = -795\n",
      "[Epoch 1] grad = -1592, x = 158405\n",
      "[Epoch 2] grad = 316808, x = -31522395\n",
      "[Epoch 3] grad = -63044792, x = 6272956805\n",
      "[Epoch 4] grad = 12545913608, x = -1248318403995\n",
      "[Epoch 5] grad = -2496636807992, x = 248415362395205\n",
      "[Epoch 6] grad = 496830724790408, x = -49434657116645595\n",
      "[Epoch 7] grad = -98869314233291192, x = 9837496766212473605\n",
      "[Epoch 8] grad = 19674993532424947208, x = -1957661856476282247195\n",
      "[Epoch 9] grad = -3915323712952564494392, x = 389574709438780167192005\n",
      "[Epoch 10] grad = 779149418877560334384008, x = -77525367178317253271208795\n",
      "[Epoch 11] grad = -155050734356634506542417592, x = 15427548068485133400970550405\n",
      "[Epoch 12] grad = 30855096136970266801941100808, x = -3070082065628541546793139530395\n",
      "[Epoch 13] grad = -6140164131257083093586279060792, x = 610946331060079767811834766548805\n",
      "[Epoch 14] grad = 1221892662120159535623669533097608, x = -121578319880955873794555118543211995\n",
      "[Epoch 15] grad = -243156639761911747589110237086423992, x = 24194085656310218885116468590099187205\n",
      "[Epoch 16] grad = 48388171312620437770232937180198374408, x = -4814623045605733558138177249429738253595\n",
      "[Epoch 17] grad = -9629246091211467116276354498859476507192, x = 958109986075540978069497272636517912465605\n",
      "[Epoch 18] grad = 1916219972151081956138994545273035824931208, x = -190663887229032654635829957254667064580655195\n",
      "[Epoch 19] grad = -381327774458065309271659914509334129161310392, x = 37942113558577498272530161493678745851550384005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37942113558577498272530161493678745851550384005"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd(5, 100, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above behaviour is also easy to understand: according to the definition of gradient, if the learning rate/step is too big, then probably we will overpass the minimum point, the value of the function will increase instead of decrease.\n",
    "\n",
    "With a bit more thinking: since smaller learning rate will make the function converge, and bigger learning rate will make the function diverge, then there should be transition point in between will make the function just move in circle.\n",
    "\n",
    "Lets still start from 5, and choose a special learning rate to make it after two iterations go back to 5 again, lets construct a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x = 5 &\\Rightarrow g(5) = 2 * 5 - 2 = 8 \\\\\n",
    "\\text{next x: }x' = x - \\alpha * g(5) = 5 - 8\\alpha &\\Rightarrow g(x') = 2 * (5 - 8\\alpha) - 2 \\\\\n",
    "\\text{next next x(and we make it be 5 again): }x'' = x' - \\alpha * g(x') = 16\\alpha^2 - 16\\alpha + 5 = 5 &\\Rightarrow \\alpha = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "That is to say, when learning rate $\\alpha = 1$, the iteration will go in circle, lets try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] grad = 8, x = -3\n",
      "[Epoch 1] grad = -8, x = 5\n",
      "[Epoch 2] grad = 8, x = -3\n",
      "[Epoch 3] grad = -8, x = 5\n",
      "[Epoch 4] grad = 8, x = -3\n",
      "[Epoch 5] grad = -8, x = 5\n",
      "[Epoch 6] grad = 8, x = -3\n",
      "[Epoch 7] grad = -8, x = 5\n",
      "[Epoch 8] grad = 8, x = -3\n",
      "[Epoch 9] grad = -8, x = 5\n",
      "[Epoch 10] grad = 8, x = -3\n",
      "[Epoch 11] grad = -8, x = 5\n",
      "[Epoch 12] grad = 8, x = -3\n",
      "[Epoch 13] grad = -8, x = 5\n",
      "[Epoch 14] grad = 8, x = -3\n",
      "[Epoch 15] grad = -8, x = 5\n",
      "[Epoch 16] grad = 8, x = -3\n",
      "[Epoch 17] grad = -8, x = 5\n",
      "[Epoch 18] grad = 8, x = -3\n",
      "[Epoch 19] grad = -8, x = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd(5, 1, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just happended like we expected.\n",
    "\n",
    "So we can conclude for cost function $f(x) = x^2 - 2x + 1$ the transition point for learning rate is 1, smaller than 1 the function will converge, bigger than 1 the function will deverge.\n",
    "\n",
    "But what is the learning rate transition point for other cost functions, lets do some other experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] grad = 36, x = -4.0\n",
      "[Epoch 1] grad = -36.0, x = 5.0\n",
      "[Epoch 2] grad = 36.0, x = -4.0\n",
      "[Epoch 3] grad = -36.0, x = 5.0\n",
      "[Epoch 4] grad = 36.0, x = -4.0\n",
      "[Epoch 5] grad = -36.0, x = 5.0\n",
      "[Epoch 6] grad = 36.0, x = -4.0\n",
      "[Epoch 7] grad = -36.0, x = 5.0\n",
      "[Epoch 8] grad = 36.0, x = -4.0\n",
      "[Epoch 9] grad = -36.0, x = 5.0\n",
      "[Epoch 10] grad = 36.0, x = -4.0\n",
      "[Epoch 11] grad = -36.0, x = 5.0\n",
      "[Epoch 12] grad = 36.0, x = -4.0\n",
      "[Epoch 13] grad = -36.0, x = 5.0\n",
      "[Epoch 14] grad = 36.0, x = -4.0\n",
      "[Epoch 15] grad = -36.0, x = 5.0\n",
      "[Epoch 16] grad = 36.0, x = -4.0\n",
      "[Epoch 17] grad = -36.0, x = 5.0\n",
      "[Epoch 18] grad = 36.0, x = -4.0\n",
      "[Epoch 19] grad = -36.0, x = 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets assume we have another cost function.\n",
    "def f1(x):\n",
    "    return 4 * x * x - 4 * x + 1\n",
    "\n",
    "# Then its corresponding derivative.\n",
    "def g1(x):\n",
    "    return 8 * x - 4\n",
    "\n",
    "gd(5, 0.25, g1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: looks there are some connections between the learning rate transition point(safety threshold) and the second derivative of the cost function.\n",
    "\n",
    "### Final question\n",
    "Must there be a learning rate transition point(safety threshold) for all kinds of cost functions?\n",
    "\n",
    "But at least in the future we can try to follow the above calculation way to try to find out the learning rate transition point(safety threshold) when we have one actual cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit to [梯度下降是门手艺活……](https://zhuanlan.zhihu.com/p/21486804)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
