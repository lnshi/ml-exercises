{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type I error, type II error, and power of test\n",
    "\n",
    "- Type 1 error: false positive, is the rejection of a `true` null hypothesis;\n",
    "\n",
    "- Type 2 error: false negative, is the failing rejection of a `false` null hypothesis;\n",
    "\n",
    "- Power of test: is defined as the probability of rejecting the null hypothesis which it is false;\n",
    "\n",
    "## Over-fitting vs under-fitting\n",
    "\n",
    "- Over-fitting usually happens when there is a small amount of the data and a large number of variables (model is over-complicated for the data), the model finished training ends up with modeling not only the information, but also the noise;\n",
    "\n",
    "- Conversely if the model is not even properly modeling the information, we call it under-fitting;\n",
    "\n",
    "- The ideal case is the model ends up modeling the information and leave out the noise;\n",
    "\n",
    "- Over-fitting can be avoided by using cross-validation (like k-folds), and regularization techniques;\n",
    "\n",
    "## When do you use the classification technique over the regression technique?\n",
    "\n",
    "- Classification problems are mainly used when the output is the categorical variable (discrete), whereas regression techniques are used the output variable is continuous variable;\n",
    "\n",
    "## What is the importance of data cleansing?\n",
    "\n",
    "- As the name suggest, data cleansing is the process of removing/updating the information that is incorrect, incomplete, duplicated, irrelevant, or formatted improperly; It is very important to improve the quality of the data, eventually leads to a reliable/more accurate model;\n",
    "\n",
    "## What are the important steps of data cleansing?\n",
    "\n",
    "- Data correctness\n",
    "\n",
    "- Removing duplicated data also irrelevant data\n",
    "\n",
    "- Structural errors\n",
    "\n",
    "- Outliers\n",
    "\n",
    "- Treatment for missing data\n",
    "\n",
    "## How is K-NN different from K-means clustering?\n",
    "\n",
    "- K-nearest neighbors is a classification/regression ML algorithm, which is a subset of supervised learning;\n",
    "\n",
    "- K-means is a clustering ML algorithm, which is a subset of unsupervised learning;\n",
    "\n",
    "- K-NN is the number of nearest neighbors used to classify or predict (in case of continuous variable/regression) a test sample, whereas K-means is the number of clusters the algorithm is trying to learn from the data;\n",
    "\n",
    "## What is p-value?\n",
    "\n",
    "- p-value helps you determine the strengths of your results when you perform a hypothesis test;\n",
    "\n",
    "- [浅谈p值（p-value是什么）](https://www.jianshu.com/p/4c9b49878f3d)\n",
    "\n",
    "- [Chi-square distribution](https://en.wikipedia.org/wiki/Chi-square_distribution#:~:text=In%20probability%20theory%20and%20statistics,independent%20standard%20normal%20random%20variables.)\n",
    "\n",
    "## What is the use of statistics in data science?\n",
    "\n",
    "- Statistics provides tools and methodologies to identify the patterns and structures in data to allow human gain a deeper insight of the data;\n",
    "\n",
    "## Supervised learning vs unsupervised learning\n",
    "\n",
    "- Supervised learning requires labeled data from training, while unsupervised learning doesn't;\n",
    "\n",
    "- Supervised learning finds application in classification and regression tasks;\n",
    "\n",
    "- Unsupervised learning finds application in clustering and association rules mining;\n",
    "\n",
    "## Explain normal distribution\n",
    "\n",
    "- Normal distribution is also called the Gaussian Distribution...\n",
    "\n",
    "## Mention some drawbacks of the linear model\n",
    "\n",
    "- I don't really agree with this question, there is no so called drawbacks for different types of models, it is just whether it suits for different kinds of problem/situation;\n",
    "\n",
    "- Linear model trains fast, and can be easily applied to big scale of data, but for some problem the data just simply don't have linear connections, but you cannot call it the drawbacks of the linear model...\n",
    "\n",
    "## What is Naive Bayes and what does the naive mean there?\n",
    "\n",
    "- Naive Bayes gives you the posterior probability of an event given what is known as prior knowledge.\n",
    "\n",
    "- A naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable.\n",
    "\n",
    "- Basically, it is  `naive` because it makes assumptions that may or may not turn out to be correct.\n",
    "\n",
    "## How can you select k for k-means?\n",
    "\n",
    "- Elbow method\n",
    "\n",
    "- Silhouette score method (Silhouette source is the most prevalent while determining the optimal value of k)\n",
    "\n",
    "## How to measure a model?\n",
    "\n",
    "- Accuracy: correctly predicted samples amount / total samples amount\n",
    "\n",
    "- Error rate incorrectly predicted samples amount / total samples amount\n",
    "\n",
    "- Sensitive: correctly predicted positive samples amount / total positive samples amount\n",
    "\n",
    "- Specificity: correctly predicted negative samples amount / total negative samples amount\n",
    "\n",
    "- Precision: true positive rate + true negative rate\n",
    "\n",
    "- Recall: is also known as the true positive rate\n",
    "\n",
    "- F-Measure: \n",
    "\n",
    "---\n",
    "\n",
    "- Inference time:\n",
    "\n",
    "- Robustness\n",
    "\n",
    "- Explainability\n",
    "\n",
    "- scalability\n",
    "\n",
    "---\n",
    "\n",
    "- ROC (Receiver operating characteristic) curve\n",
    "\n",
    "- PR (precision recall) curve\n",
    "\n",
    "## What are lambda functions?\n",
    "\n",
    "- A lambda function is a small anonymous function.\n",
    "\n",
    "- A lambda function can take any number of arguments, but can only have one expression.\n",
    "\n",
    "## What is reinforcement learning?\n",
    "\n",
    "- Reinforcement learning is an unsupervised learning technique in machine learning.\n",
    "\n",
    "- It is a state-based learning technique. The models have predefined rules for state change which enable the system to move from one state to another, while the training phase.\n",
    "\n",
    "## What is entropy and information gain in decision tree algorithm?\n",
    "\n",
    "- Entropy is used to check the homogeneity of a sample.\n",
    "\n",
    "- If the value of entropy is `0` then the sample is completely homogeneous. On the other hand, if entropy has a value `1`, the sample is equally divided. Entropy controls how a Decision Tree decides to split the data. It actually affects how a Decision Tree draws its boundaries.\n",
    "\n",
    "- The information gain depends on the decrease in entropy after the dataset is split on an attribute. Constructing a decision tree is always about finding the attributes that return highest information gain.\n",
    "\n",
    "## What is cross-validation?\n",
    "\n",
    "- Training dataset + testing dataset\n",
    "\n",
    "- LOOCV (leave one out cross validation): like the simple training dataset + testing dataset partition, but each step/epoch we only use one data as the test data, and repeat this n times;\n",
    "\n",
    "- K-fold cross validation: like the LOOCV but not using one data as the test data, instead of using one of the k-fold as test data each epoch, repeat k times;\n",
    "\n",
    "- Training dataset + validation dataset + testing dataset;\n",
    "\n",
    "## What is bias-variance trade off?\n",
    "\n",
    "- Bias, variance are all concepts to the generalization problem in ML training, usually we will define a loss function, and then try to minimize it during the training, but purely minimizing the loss doesn't equal to the model will perform well in the more generalized/real data, with low loss the model even can be totally NOT usable when face the more generalized/real data; the gap between the loss of the training dataset and the loss of the validation/testing dataset is called generalization error;\n",
    "\n",
    "- Generalization error basically has 3 types: random error, bias, and variance;\n",
    "\n",
    "- Error = $bias^2 + variance$\n",
    "\n",
    "- Random error is caused by the noise of the data, the outliers, etc, it is hardly be avoided;\n",
    "\n",
    "- `Bias` is the gap between the expected value of the predictions from the trained model and the real value, basically it tells the __precision__ of the model;\n",
    "\n",
    "- `Variance` measures how far a set of predictions are spread out from above the trained model's expected value, basically it tells the __stabilities__ of the model;\n",
    "\n",
    "- Take the target shotting game as example, after trained, one buy with his favorite gun each shoot expected to be 9 (10 is the full score), but one shoot he got 7, here: the bias is 10 - 9 = 1, the variance is 9 - 7 = 2;\n",
    "\n",
    "- Thus if your model is over-simplified for the dataset it usually leads to high bias + low variance, conversely if your model is over-complicated for the dataset it usually leads to low bias + high variance, which both are not what we want, tuning/adjust the model to balance between the bias and variance to best solve the problem is the bias-variance trade off.\n",
    "\n",
    "- [谈谈 Bias-Variance Tradeoff](https://liam.page/2017/03/25/bias-variance-tradeoff/)\n",
    "\n",
    "## The types of biases that occur during sampling\n",
    "\n",
    "- Self-selection bias\n",
    "\n",
    "- Under-coverage bias \n",
    "\n",
    "- Survivorship bias\n",
    "\n",
    "- [Sampling bias: What is it and why does it matter?](https://www.scribbr.com/methodology/sampling-bias/)\n",
    "\n",
    "## What is the Confusion Matrix?\n",
    "\n",
    "- A binary classifier predicts all data instances of a test dataset as either positive or negative. This produces four outcomes:\n",
    "    * True positive(TP) — Correct positive prediction\n",
    "    * False-positive(FP, type 1 error) — Incorrect positive prediction\n",
    "    * True negative(TN) — Correct negative prediction\n",
    "    * False-negative(FN, type 2 error) — Incorrect negative prediction\n",
    "    \n",
    "- It helps in calculating various measures including:\n",
    "    * error rate: $(FP + FN)/(P + N)$\n",
    "    * specificity: $(TN / N)$\n",
    "    * accuracy: $(TP + TN)/(P + N)$\n",
    "    * sensitivity: $(TP / P)$\n",
    "    * precision: $(TP / (TP + FP))$\n",
    "    \n",
    "- A confusion matrix is essentially used to evaluate the performance of a machine learning model;\n",
    "\n",
    "## What are exploding/vanishing gradients?\n",
    "\n",
    "- Exploding/vanishing gradients is the problematic scenario where large/small error gradients accumulate to result in very large/small updates to the weights of neural network models in the training stage. Hence the model becomes unstable and is unable to learn from the training data;\n",
    "\n",
    "- Can be avoided via changing the initialization value, or use a different activation functions, or doing the data normalization;\n",
    "\n",
    "## What is Law of Large Numbers?\n",
    "\n",
    "- The `Law of Large Numbers` states that if an experiment is repeated independently a large number of times, the average of the individual results is close to the expected value. It also states that the sample variance and standard deviation also converge towards the expected value.\n",
    "\n",
    "## What is the importance of A/B testing?\n",
    "\n",
    "- It helps to pick the best variant among multiple hypotheses fairly;\n",
    "\n",
    "## What is eigenvectors and eigenvalues?\n",
    "\n",
    "- Eigenvectors depict the direction in which a linear transformation moves and acts by compressing, flipping, or stretching. They are used to understand linear transformations and are generally calculated for a correlation or covariance matrix.\n",
    "\n",
    "- The eigenvalue is the strength of the transformation in the direction of the eigenvector. \n",
    "\n",
    "- An eigenvector’s direction remains unchanged when a linear transformation is applied to it.\n",
    "\n",
    "- [如何理解矩阵特征值和特征向量？](https://www.matongxue.com/madocs/228/)\n",
    "\n",
    "## What is systematic sampling and cluster sampling\n",
    "\n",
    "- Systematic sampling is a type of probability sampling method. The sample members are selected from a larger population with a random starting point but a fixed periodic interval. This interval is known as the sampling interval. The sampling interval is calculated by dividing the population size by the desired sample size.\n",
    "\n",
    "- Cluster sampling involves dividing the sample population into separate groups, called clusters. Then, a simple random sample of clusters is selected from the population.\n",
    "\n",
    "## What are AutoEncoders?\n",
    "\n",
    "- An AutoEncoder is a kind of artificial neural network. It is used to learn efficient data codings in an unsupervised manner. It is utilized for learning a representation (encoding) for a set of data, mostly for dimensionality reduction, by training the network to ignore signal noise.\n",
    "\n",
    "- You can think it as an advanced version of PCA (because of its non-linear transformation units);\n",
    "\n",
    "- Nowadays AutoEncoder also expands to generate a representation as close as possible to its original input from the reduced encoding.\n",
    "\n",
    "- Sparse AutoEncoder\n",
    "\n",
    "- Denoising AutoEncoder\n",
    "\n",
    "- Variational AutoEncoder: the famous hand written digits recognition dataset is generated by using this technique;\n",
    "\n",
    "- [当我们在谈论 Deep Learning：AutoEncoder 及其相关模型](https://zhuanlan.zhihu.com/p/27865705)\n",
    "\n",
    "## How do you avoid the over-fitting during the training?\n",
    "\n",
    "- Keeping the model simple\n",
    "\n",
    "- Using cross validation\n",
    "\n",
    "- Using regularization to penalize the model parameters that are more likely to cause the over-fitting\n",
    "    * L1 regularization: $\\sum\\limits_{i=1}^n|y_i - f(x_i)|$\n",
    "    * L2 regularization: $\\sum\\limits_{i=1}^n\\big(y_i - f(x_i)\\big)^2$\n",
    "    \n",
    "- Why usually L2 is more popular than L1: because of the related calculation is more convenient (derivation)\n",
    "\n",
    "- [l1正则与l2正则的特点是什么，各有什么优势？](https://www.zhihu.com/question/26485586/answer/616029832)\n",
    "\n",
    "    * In a nutshell, why L2 regularization is more popular then L1 is because of the derivation computing conveniences (because for an equation with absolute items its derivation is not continuous)!\n",
    "\n",
    "## What are the differences among standardization, normalization and regularization?\n",
    "\n",
    "- Standardization: ?\n",
    "\n",
    "- Normalization: $x_{new} = \\frac{x - \\mu}{\\sigma}$ $\\mu = \\text{Mean and } \\sigma = \\text{Standard Deviation}$, no matter the data we’re working with, after normalizing it, the mean will be equal to 0 and the standard deviation will be equal to 1; Refer here: [Batch Normalization Tensorflow Keras Example](https://towardsdatascience.com/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e)\n",
    "\n",
    "- Regularization: penalty regularization items (L1, L2)\n",
    "\n",
    "## What is dimensionality reduction? What are its benefits?\n",
    "\n",
    "- Dimensionality reduction is defined as the process of converting a data set with vast dimensions into data with lesser dimensions — in order to convey similar information concisely. \n",
    "\n",
    "- This method is mainly beneficial in compressing data and reducing storage space. It is also useful in reducing computation time due to fewer dimensions. Finally,  it helps remove redundant features — for instance, storing a value in two different units (meters and inches) is avoided.\n",
    "\n",
    "## Some feature selection methods used to select the right variables\n",
    "\n",
    "- The methods for feature selection can be broadly classified into two types:\n",
    "    * Filter Methods, these methods involve:\n",
    "        - Linear discrimination analysis\n",
    "        - ANOVA\n",
    "        - Chi-Square\n",
    "    * Wrapper Methods, these methods involve:\n",
    "        - Forward Selection: one feature at a time is tested, and a good fit is obtained;\n",
    "        - Backward Selection: all features are reviewed to see what works better;\n",
    "        - Recursive Feature Elimination: every different feature is looked at recursively and paired together accordingly. \n",
    "    * Others are Forward Elimination, Backward Elimination for Regression, Cosine Similarity-Based Feature Selection for Clustering tasks, Correlation-based eliminations etc.\n",
    "    \n",
    "## What are the different types of clustering algorithms?\n",
    "\n",
    "- K-NN\n",
    "\n",
    "- Hierarchical clustering\n",
    "\n",
    "- Fuzzy clustering\n",
    "\n",
    "## MAE vs MSE vs RMSE\n",
    "\n",
    "- MAE: mean absolute error\n",
    "\n",
    "- MSE: mean square error\n",
    "\n",
    "- RMSE: $\\sqrt{MSE}$\n",
    "\n",
    "## How can outlier values be treated?\n",
    "\n",
    "- Replace the value with mean, mode (__mode is a statistical term that refers to the most frequently occurring number found in a set of numbers__)\n",
    "\n",
    "- Treat as missing value;\n",
    "\n",
    "- Remove entire record;\n",
    "\n",
    "## What is skewed distribution and uniform distribution\n",
    "\n",
    "- The skewed distribution is a distribution in which the majority of the data points lie to the right or left of the center (left skewed distribution, right skewed distribution, they are all variants of normal distribution);\n",
    "\n",
    "- A uniform distribution is a probability distribution in which all outcomes are equally likely.\n",
    "\n",
    "## What is a Box-Cox Transformation?\n",
    "\n",
    "- Box-Cox transformation is a way to normalize variables.\n",
    "\n",
    "## What is the hyperbolic tree?\n",
    "\n",
    "- Also known as hyper-tree;\n",
    "\n",
    "- Displaying hierarchical data as a tree suffers from visual clutter as the number of nodes per level can grow exponentially.\n",
    "\n",
    "## How to deal with imbalanced dataset?\n",
    "\n",
    "The imbalanced dataset should be identified as below two cases:\n",
    "\n",
    "1. The less part still covers the realities very well\n",
    "\n",
    "2. The less part cannot cover the realities very well\n",
    "\n",
    "If `1` is the case, under-sampling + using proper measurement metrics (confusion matrix, precision, and F1) should solve the problem already;\n",
    "\n",
    "If `2` is the case, either reworking the dataset or reworking the problem itself (design different solution)\n",
    "\n",
    "- ~~Undersampling consists in sampling from the majority class in order to keep only a part of these points~~\n",
    "\n",
    "- Collecting more data\n",
    "\n",
    "- Oversampling consists in replicating some points from the minority class in order to increase its cardinality\n",
    "\n",
    "- Generating synthetic data consists in creating new synthetic points from the minority class (see SMOTE method for example) to increase its cardinality\n",
    "\n",
    "[Handling imbalanced datasets in machine learning](https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28)\n",
    "\n",
    "[SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "\n",
    "##  What’s the difference between probability and likelihood?\n",
    "\n",
    "- [What is the difference between “likelihood” and “probability”?](https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability#2647)\n",
    "\n",
    "## What cross-validation technique would you use on a time series dataset?\n",
    "\n",
    "- [Using k-fold cross-validation for time-series model selection](https://stats.stackexchange.com/a/14109/229537)\n",
    "\n",
    "- Instead of using standard k-folds cross-validation, you have to pay attention to the fact that a time series is not randomly distributed data — it is inherently ordered by chronological order. If a pattern emerges in later time periods for example, your model may still pick up on it even if that effect doesn’t hold in earlier years!\n",
    "\n",
    "- You’ll want to do something like forward chaining where you’ll be able to model on past data then look at forward-facing data.\n",
    "    * fold 1 : training [1], test [2]\n",
    "    * fold 2 : training [1 2], test [3]\n",
    "    * fold 3 : training [1 2 3], test [4]\n",
    "    * fold 4 : training [1 2 3 4], test [5]\n",
    "    * fold 5 : training [1 2 3 4 5], test [6]\n",
    "    \n",
    "## How is a decision tree pruned? \n",
    "\n",
    "- [Decision tree pruning](https://en.wikipedia.org/wiki/Decision_tree_pruning)\n",
    "\n",
    "## What’s the F1 score? How would you use it?\n",
    "\n",
    "- The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives don’t matter much.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "523px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
