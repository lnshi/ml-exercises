## Project GitHub Pages

- [Leonard's Machine Learning Exercises](https://lnshi.github.io/ml-exercises/)

## Topics

### ml_basics

- [Multivariable linear regression(gradient descent)](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm001_multivariable_linear_regression_gradient_descent/multivariable_linear_regression_gradient_descent.html)

- <p>
    <details>
      <summary>
        <a href="https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm002_gradient_and_gradient_descent/gradient_and_gradient_descent.html">
          Gradient and gradient descent
        </a>
      </summary>
      <p>
        <ul>
          <li>Derivative</li>
          <li>Derivative and partial derivative</li>
          <li>Derivative and directional derivative</li>
          <li>Derivative and gradient</li>
          <li>Gradient descent algorithm</li>
        </ul>
      </p>
    </details>
  </p>

- [Gradient descent learning rate chosen](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm003_gradient_descent_learning_rate_chosen/gradient_descent_learning_rate_chosen.html)

- <p>
    <details>
      <summary>
        <a href="https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm004_normal_equation/normal_equation.html">
          Normal equation
        </a>
      </summary>
      <p>
        <ul>
          <li>Vector addition and subtraction</li>
          <li>Vector dot product (scalar product, inner product)</li>
          <li>Vector cross product</li>
          <li>Normal equation</li>
        </ul>
      </p>
    </details>
  </p>

- [PDF vs PMF vs CDF](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm005_PDF_PMF_CDF/PDF_PMF_CDF.html)

- [Bayes’ Theorem and MLE MAP](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm006_Bayes%E2%80%99%20Theorem_and_MLE_MAP/Bayes%E2%80%99%20Theorem_and_MLE_MAP.html)

- <p>
    <details>
      <summary>
        <a href="https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html">
          Logistic regression (binomial regression) and regularization
        </a>
      </summary>
      <p>
        <ul>
          <li>Experience scipy.optimize.fmin_tnc</li>
          <li>Regularization</li>
          <li>Norm of vector and matrix</li>
          <li>Dataset features expansion/extraction</li>
          <li>
            When a lower dimensional space NOT discriminable dataset is PROJECTED to a PROPER higher dimensional space it always will be discriminable, the boundary is a hyper plane or just a discrimination function.
          </li>
          <li>
            Model accuracy comparison between 10-dimensional and 6-dimensional
          </li>
          <li>'linear_model.LogisticRegression' with sklearn</li>
        </ul>
      </p>
    </details>
  </p>

- <p>
    <details>
      <summary>
        <a href="https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm008_GLM_and_exponential_family_distributions/GLM_and_exponential_family_distributions.html">
          GLM and exponential family distributions
        </a>
      </summary>
      <p>
        <ul>
          <li>Bernoulli distribution in GLM form</li>
          <li>Gaussian distribution (normal distribution) in GLM form</li>
          <li>
            Softmax regression (multinomial logistic regression) (categorical distribution (variant 3)) in GLM form
          </li>
          <li>GLM ⇒ linear regression</li>
          <li>GLM ⇒ logistic regression</li>
          <li>
            <a href="https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm008_GLM_and_exponential_family_distributions/GLM_and_exponential_family_distributions.html#Why-the-PMF-has-no-coefficient?">
              Why the PMF for categorical distribution(special form of multinomial distribution: k > 2 and n = 1) has no coefficient like the multinomial distribution's PMF
            </a>
          </li>
          <li>
            How to use the table
            <a href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">
              here
            </a>
            to
            <a href="https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm008_GLM_and_exponential_family_distributions/GLM_and_exponential_family_distributions.html#With-above-three-hypotheses,-GLM-$\Rightarrow$-logistic-regression">
              build GLM quickly
            </a>
          </li>
        </ul>
      </p>
    </details>
  </p>

- [Multinomial logistic regression](https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm009_multinomial_logistic_regression/multinomial_logistic_regression.ipynb?flush_cache=true)

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm010_neural_network/neural_network.ipynb?flush_cache=true">
          Neural network (hand-written digits recognition)
        </a>
      </summary>
      <p>
        <ul>
          <li>What's a neural network</li>
          <li>One-hot encoding</li>
          <li>Forward propagation</li>
          <li>Backpropagation algorithm</li>
        </ul>
      </p>
    </details>
  </p>

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm011_support_vector_machines/support_vector_machines.ipynb?flush_cache=true">
          SVM
        </a>
      </summary>
      <p>
        <ul>
          <li>Functional margin vs geometric margin, and SVM interpretation</li>
          <li>Lagrange multiplier, Lagrange duality and KKT conditions</li>
          <li>Coordinate ascent algorithm and SMO</li>
          <li>Slack variables and penalty factors</li>
        </ul>
      </p>
    </details>
  </p>

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm012_principal_components_analysis/principal_components_analysis.ipynb?flush_cache=true">
          PCA
        </a>
      </summary>
      <p>
        <ul>
          <li>What does PCA do?</li>
          <li>Another interpretation of what does PCA do</li>
          <li>What are the application scenarios of PCA?</li>
          <li>PCA pseudo code</li>
          <li>Eigenvalues and eigenvectors</li>
          <li>Singular value and Singular Value Decomposition</li>
          <li>PCA dimensionality reduction example</li>
        </ul>
      </p>
    </details>
  </p>

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm013_some_generic_questions/some_generic_questions.ipynb?flush_cache=true">
          Some generic questions
        </a>
      </summary>
      <p>
        <ul>
          <li>1. Type 1 error, type 2 error, power of test</li>
          <li>2. Over-fitting, under-fitting</li>
          <li>6. How is k-NN different from k-means clustering?</li>
          <li>7. p-value</li>
          <li>12. What is naive in a Naive Bayes?</li>
          <li>13. How can you select k for k-means?</li>
          <li>14. How to measure a model?</li>
          <li>18. What is cross-validation?</li>
          <li>19. What is bias-variance trade off?</li>
          <li>20. The types of biases that occur during sampling?</li>
          <li>21. What is the confusion matrix?</li>
          <li>22. What are exploding/vanishing gradients?</li>
          <li>25. What is eigenvectors and eigenvalues?</li>
          <li>27. What are AutoEncorders?</li>
          <li>28. How do you avoid the over-fitting during the training?</li>
          <li>29. What are the differences among standardization, normalization and regularization?</li>
          <li>30. Some feature selection methods used to select the right variables</li>
          <li>33. MAE vs MSE vs RMSE</li>
          <li>34. How can outlier values be treated?</li>
          <li>36. What is a Box-Cox Transformation?</li>
          <li>37. What is the hyperbolic tree?</li>
          <li>39. What's the difference between probability and likelihood?</li>
          <li>40. What cross-validation technique would you use on a time series dataset?</li>
        </ul>
      </p>
    </details>
  </p>

- [Decision tree and random forest](https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm014_decision_tree_and_random_forest/decision_tree_and_random_forest.ipynb?flush_cache=true)

- [k-fold cross validation sklearn example](https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm015_k-fold_cross_validation_sklearn_example/k-fold_cross_validation_sklearn_example.ipynb?flush_cache=true)

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm016_ensemble_learning/ensemble_learning.ipynb?flush_cache=true">
          Ensemble learning
        </a>
      </summary>
      <p>
        <ul>
          <li>What is bootstrapping (bootstrap sampling)?</li>
          <li>Why do we need bootstrap sampling?</li>
          <li>Implement Bootstrap Sampling in Python</li>
          <li>What is ensemble learning</li>
          <li>Boosting</li>
          <li>Bagging</li>
          <li>Stacking</li>
        </ul>
      </p>
    </details>
  </p>

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm017_gradient-boost(GBM)_vs_ada-boost_xgboost/gradient-boost(GBM)_vs_ada-boost_xgboost.ipynb?flush_cache=true">
          Gradient(GBM) vs AdaBoost vs XGBoost
        </a>
      </summary>
      <p>
        <ul>
          <li>Decision tree vs random forest</li>
          <li>Random forest vs gradient boosting</li>
          <li>Gradient vs AdaBoost</li>
          <li>XGBoost</li>
        </ul>
      </p>
    </details>
  </p>

- [RNN and LSTM](https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm018_RNN_and_LSTM/RNN_and_LSTM.ipynb?flush_cache=true)

- [Model parameters vs hyperparameters](https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm019_model_parameters_vs_hyperparameters/model_parameters_vs_hyperparameters.ipynb?flush_cache=true)

- <p>
    <details>
      <summary>
        <a href="https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm020_optimizers/optimizers.ipynb?flush_cache=true">
          Optimizers
        </a>
      </summary>
      <p>
        <ul>
          <li>Gradient descent</li>
          <li>Types of gradient descent</li>
          <li>Role of an optimizer</li>
          <li>Types of optimizers</li>
          <li>Momentum</li>
          <li>Nesterov accelerated gradient(NAG)</li>
          <li>Adagrad — Adaptive Gradient Algorithm</li>
          <li>Adadelta</li>
          <li>RMSProp</li>
          <li>Adam — Adaptive Moment Estimation</li>
          <li>Nadam- Nesterov-accelerated Adaptive Moment Estimation</li>
        </ul>
      </p>
    </details>
  </p>

## Questions

1. [In gradient descent, must there be a learning rate transition point(safety threshold) for all kinds of cost functions?](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm003_gradient_descent_learning_rate_chosen/gradient_descent_learning_rate_chosen.html#Final-question)

2. [How do we extend this to the cross product of a four dimensional vector or more higher, like the right part of the above graph?](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm004_normal_equation/normal_equation.html#Cross-product)

3. [When a lower dimensional space NOT discriminable dataset is PROJECTED to a PROPER higher dimensional space it always will be discriminable, the boundary is a hyper plane or just a discrimination function, what are the differences of the 'a hyper plane' or 'a discrimination function' here?](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#Question:-what-are-the-differences-of-the-'a-hyper-plane'-or-'a-discrimination-function'-here?)

4. [What are the best practices / skills / underlying theories for the features expansion/extraction?](https://lnshi.github.io/ml-exercises/ml_basics_in_html/rdm007_logistic_regression%28binomial_regression%29_and_regularization/logistic_regression%28binomial_regression%29_and_regularization.html#Question:-what-are-the-best-practices-/-skills-/-underlying-theories-for-the-features-expansion/extraction?)

5. [For the hand-written digits recognition NN why the hidden layer input size is 25? How about design to use a different size? What is the theory/best practices/skills/prior knowledges here?](https://nbviewer.jupyter.org/github/lnshi/ml-exercises/blob/master/ml_basics/rdm010_neural_network/neural_network.ipynb?flush_cache=true#Question:-why-the-hidden-layer-has-25-units?)

## Accumulations / References

1. [如何理解最小二乘法？](https://mp.weixin.qq.com/s/4e9ZiiGIOWx_ZUGjzgavWw)

2. `np.array([0, 0])` vs `np.array([0., 0.])`

    ```
    >>> import numpy as np
    >>> t = np.array([0, 0])
    >>> t[0] = 0.97
    >>> t
    array([0, 0])
    >>> t[0] = 1.97
    >>> t
    array([1, 0])
    >>> t = np.array([0., 0.])
    >>> t[0] = 0.97
    >>> t
    array([0.97, 0.  ])
    >>> t = np.array([0, 0.])
    >>> t[0] = 0.97
    >>> t
    array([0.97, 0.  ])
    ```

## Memos

1. [Exponential family form of multinomial distribution <- my answer -> link to topic 'GLM and exponential family distributions'](https://stats.stackexchange.com/a/380950/229537)

2. [Batch Normalization Tensorflow Keras Example](https://towardsdatascience.com/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e)
